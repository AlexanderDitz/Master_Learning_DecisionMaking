import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

plt.style.use('default')
plt.rcParams['figure.facecolor'] = 'white'
plt.rcParams['axes.facecolor'] = 'white'


csv_path       = 'AAAAsindy_analysis_with_metrics.csv' 
min_k          = 2     # minimum k to test
max_k          = 8     # maximum k to test
n_pca_comp     = 7     # number of PCA components to keep
random_state   = 42     

output_dir = '/Users/martynaplomecka/closedloop_rl/analysis/participants_analysis_plots/sindy_coefficients_analysis/sindy_clusters'
os.makedirs(output_dir, exist_ok=True)

def load_and_prepare_data(csv_path):


    df = pd.read_csv(csv_path)

    # 1a) All columns starting with "x_"
    all_sindy_cols = [c for c in df.columns if c.startswith('x_')]

    # 1b) Split into linear / quadratic / interaction
    linear_terms      = [c for c in all_sindy_cols if c.endswith('_1')]
    quadratic_terms   = [c for c in all_sindy_cols if ('_x_' in c and not c.endswith('_1'))]
    interaction_terms = [c for c in all_sindy_cols if ('_c_' in c and not c.endswith('_1'))]

    # 1c) Beta parameters (not used for clustering)
    beta_columns = [c for c in ['beta_reward', 'beta_choice'] if c in df.columns]

    # 1d) Param-counts
    param_count_columns = [c for c in df.columns if c.startswith('params_')]

    # 1e) Behavioral metrics
    behavioral_columns = [
        c for c in ['switch_rate', 'stay_after_reward', 'perseveration', 'avg_reward']
        if c in df.columns
    ]

    linear_terms        = [c for c in linear_terms        if c in df.columns]
    quadratic_terms     = [c for c in quadratic_terms     if c in df.columns]
    interaction_terms   = [c for c in interaction_terms   if c in df.columns]
    param_count_columns = [c for c in param_count_columns if c in df.columns]
    behavioral_columns  = [c for c in behavioral_columns  if c in df.columns]

    return df, {
        'linear':       linear_terms,
        'quadratic':    quadratic_terms,
        'interaction':  interaction_terms,
        'beta':         beta_columns,
        'param_counts': param_count_columns,
        'behavioral':   behavioral_columns,
        'all_sindy':    linear_terms + quadratic_terms + interaction_terms
    }

df, columns_dict = load_and_prepare_data(csv_path)

sindy_cols = columns_dict['all_sindy']

# Set random seed for numpy operations
np.random.seed(random_state)

X_sindy = df[sindy_cols].fillna(0).values
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_sindy)

## (3) PCA VARIANCE‐EXPLAINED PLOT
# Fit PCA with as many components as possible to compute explained variance , turns out that 7 retains >90 pro
max_comps = min(X_scaled.shape[0], X_scaled.shape[1])
pca_full = PCA(n_components=max_comps, random_state=random_state)
pca_full.fit(X_scaled)

explained_ratios = pca_full.explained_variance_ratio_
cum_explained = np.cumsum(explained_ratios)

plt.figure(figsize=(6, 4))
plt.plot(range(1, len(cum_explained) + 1), cum_explained, '-o', color='#1f77b4')
plt.xlabel('Number of PCA Components')
plt.ylabel('Cumulative Explained Variance')
plt.title('PCA Explained Variance vs. # Components')
plt.xticks(np.arange(1, len(cum_explained) + 1, max(1, len(cum_explained)//10)))
plt.ylim(0, 1.05)
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig(os.path.join(output_dir, 'pca_variance_explained.png'), dpi=300, bbox_inches='tight')
plt.close()

## (4) PCA → REDUCE DIMENSIONALITY TO n_pca_comp
n_components = min(n_pca_comp, X_scaled.shape[1])
pca = PCA(n_components=n_components, random_state=random_state)
X_pca = pca.fit_transform(X_scaled)

print(f"PCA reduced SINDY from {X_scaled.shape[1]} dims → {n_components} dims.")

## (5) ELBOW & SILHOUETTE ANALYSIS (to pick k)
inertias        = []
silhouette_vals = []

for k in range(min_k, max_k + 1):
    km = KMeans(n_clusters=k, random_state=random_state)
    labels_k = km.fit_predict(X_pca)
    inertias.append(km.inertia_)
    if k >= 2:
        sil_score = silhouette_score(X_pca, labels_k, random_state=random_state)
        silhouette_vals.append(sil_score)
    else:
        silhouette_vals.append(np.nan)

# (5a) Plot & Save Elbow: inertia vs. k
plt.figure(figsize=(6, 4))
ks = list(range(min_k, max_k + 1))
plt.plot(ks, inertias, '-o', color='#ff7f0e')
plt.xticks(ks)
plt.xlabel('Number of clusters (k)')
plt.ylabel('KMeans Inertia')
plt.title('Elbow Plot (Inertia vs k)')
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig(os.path.join(output_dir, 'elbow_plot.png'), dpi=300, bbox_inches='tight')
plt.close()

# (5b) Plot & Save Silhouette: silhouette vs. k
plt.figure(figsize=(6, 4))
plt.plot(ks, silhouette_vals, '-o', color='#2ca02c')
plt.xticks(ks)
plt.xlabel('Number of clusters (k)')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Score vs k')
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig(os.path.join(output_dir, 'silhouette_plot.png'), dpi=300, bbox_inches='tight')
plt.close()

# After inspecting, we choose best k. best seems to be 6, second best is 3 :
best_k = 3

## (6) FINAL KMEANS WITH chosen k
kmeans = KMeans(n_clusters=best_k, random_state=random_state)
labels = kmeans.fit_predict(X_pca)
df['Cluster'] = labels

cluster_counts = df['Cluster'].value_counts().sort_index()
print("\nCluster sizes (after final k-means):")
for cl in cluster_counts.index:
    print(f"  Cluster {cl}: {cluster_counts.loc[cl]} participants")

## (7) CHARACTERIZE CLUSTERS BY AGE & BEHAVIOR
profile_cols = ['Age'] + columns_dict['behavioral']
profile_cols = [c for c in profile_cols if c in df.columns]
if len(profile_cols) < 2:
    raise RuntimeError("Need at least 'Age' plus one behavioral column to profile clusters.")

cluster_profiles = df.groupby('Cluster')[profile_cols].mean().sort_index()

print("\nCluster‐wise MEANS (Age + Behavior):")
print(cluster_profiles)

# Normalize each column to [0,1] across clusters for heatmap / radar
norm_profiles = cluster_profiles.copy()
for col in profile_cols:
    col_min = norm_profiles[col].min()
    col_max = norm_profiles[col].max()
    if col_max > col_min:
        norm_profiles[col] = (norm_profiles[col] - col_min) / (col_max - col_min)
    else:
        norm_profiles[col] = 0.5

## (8) PLOT & SAVE: BAR CHART OF RAW MEANS
n_vars = len(profile_cols)
x = np.arange(best_k)
width = 0.12
offsets = np.linspace(-width*(n_vars-1)/2, width*(n_vars-1)/2, n_vars)

plt.figure(figsize=(10, 5))
colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']  # Consistent color palette
for i, col in enumerate(profile_cols):
    values = cluster_profiles[col].values
    plt.bar(x + offsets[i], values, width=width, label=col, alpha=0.7, color=colors[i % len(colors)])

plt.xlabel('Cluster')
plt.ylabel('Mean Value')
plt.title('Cluster Means: Age & Behavioral Metrics')
plt.xticks(x, [f'Cl {i}' for i in x])
plt.legend(ncol=2, bbox_to_anchor=(1.02, 1), loc='upper left')
plt.grid(alpha=0.3, axis='y')
plt.tight_layout()
plt.savefig(os.path.join(output_dir, 'cluster_means_bar.png'), dpi=300, bbox_inches='tight')
plt.close()

## (8b) PLOT & SAVE: BAR CHART OF NORMALIZED MEANS
n_vars = len(profile_cols)
x = np.arange(best_k)
width = 0.12
offsets = np.linspace(-width*(n_vars-1)/2, width*(n_vars-1)/2, n_vars)

plt.figure(figsize=(10, 5))
for i, col in enumerate(profile_cols):
    values = norm_profiles[col].values
    plt.bar(x + offsets[i], values, width=width, label=col, alpha=0.7, color=colors[i % len(colors)])

plt.xlabel('Cluster')
plt.ylabel('Normalized Value (0-1)')
plt.title('Cluster Profiles: Normalized Age & Behavioral Metrics')
plt.xticks(x, [f'Cl {i}' for i in x])
plt.legend(ncol=2, bbox_to_anchor=(1.02, 1), loc='upper left')
plt.grid(alpha=0.3, axis='y')
plt.ylim(0, 1.1)
plt.tight_layout()
plt.savefig(os.path.join(output_dir, 'cluster_normalized_bar.png'), dpi=300, bbox_inches='tight')
plt.close()

## (9) PLOT & SAVE: HEATMAP OF NORMALIZED PROFILES
plt.figure(figsize=(6, 4))
sns.heatmap(
    norm_profiles,
    annot=True,
    cmap='RdBu_r',
    center=0.5,
    cbar_kws={'label': 'Normalized (0→1)'},
    linewidths=0.5,
    linecolor='gray'
)
plt.title('Cluster Profiles (Normalized)\n(Age & Behavioral)')
plt.ylabel('Cluster')
plt.xlabel('Variable')
plt.tight_layout()
plt.savefig(os.path.join(output_dir, 'cluster_profiles_heatmap.png'), dpi=300, bbox_inches='tight')
plt.close()

## (10) PLOT & SAVE: RADAR PLOT
def plot_radar(df_radar, categories, title, out_path):
    """
    df_radar: DataFrame indexed by cluster label; columns = categories (all in [0,1])
    categories: list of column names in the same order
    out_path: filepath to save the radar PNG
    """
    N = len(categories)
    angles = np.linspace(0, 2 * np.pi, N, endpoint=False).tolist()
    angles += angles[:1]   # close loop

    fig, ax = plt.subplots(figsize=(6, 6), subplot_kw={'polar': True})
    for idx, row in df_radar.iterrows():
        values = row[categories].tolist()
        values += values[:1]
        ax.plot(angles, values, marker='o', label=f'Cluster {idx}')
        ax.fill(angles, values, alpha=0.25)

    ax.set_thetagrids(np.degrees(angles[:-1]), categories)
    ax.set_title(title, y=1.10)
    ax.set_rlim(0, 1)
    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))
    plt.tight_layout()
    plt.savefig(out_path, dpi=300, bbox_inches='tight')
    plt.close()

radar_outfile = os.path.join(output_dir, 'cluster_profiles_radar.png')
plot_radar(norm_profiles, profile_cols, 'Cluster Profiles (Normalized)', radar_outfile)

## (11) PLOT & SAVE: PCA SCATTER COLORED BY CLUSTER
plt.figure(figsize=(6, 6))
cluster_colors = ['#1f77b4', '#ff7f0e', '#2ca02c']  # Consistent with previous plots
for cl in sorted(df['Cluster'].unique()):
    mask = df['Cluster'] == cl
    plt.scatter(X_pca[mask, 0], X_pca[mask, 1],
                alpha=0.6, label=f'Cl {cl}', color=cluster_colors[cl % len(cluster_colors)])
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.title('PCA Scatter of SINDY Features (by Cluster)')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig(os.path.join(output_dir, 'pca_scatter.png'), dpi=300, bbox_inches='tight')
plt.close()

## (12) WHICH SINDY FEATURES "DOMINATE" EACH CLUSTER?
abs_df = df[sindy_cols].abs()
abs_df['Cluster'] = df['Cluster']
cluster_abs_means = abs_df.groupby('Cluster').mean()

# Create subplots for all clusters in one figure
fig, axes = plt.subplots(1, best_k, figsize=(15, 5))
if best_k == 1:
    axes = [axes]  # Handle case where there's only one cluster

for i, cl in enumerate(cluster_abs_means.index):
    top5 = cluster_abs_means.loc[cl].sort_values(ascending=False).head(5)
    print(f"\nCluster {cl} top 5 SINDY features by mean |coef|:")
    for feat, val in top5.items():
        clean_name = feat.replace('x_', '').replace('_', ' ')
        print(f"   {clean_name:30s}  → {val:.3f}")

    # Plot in subplot
    axes[i].barh(
        y=[f.replace('x_', '').replace('_', ' ') for f in top5.index[::-1]],
        width=top5.values[::-1],
        color=cluster_colors[cl % len(cluster_colors)],
        alpha=0.7
    )
    axes[i].set_xlabel('Mean |Coefficient|')
    axes[i].set_title(f'Cluster {cl}: Top 5 SINDY Coefs')
    axes[i].grid(alpha=0.3)

plt.tight_layout()
outfile = os.path.join(output_dir, 'all_clusters_top5_sindy.png')
plt.savefig(outfile, dpi=300, bbox_inches='tight')
plt.close()