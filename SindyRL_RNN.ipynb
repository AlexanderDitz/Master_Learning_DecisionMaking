{"cells":[{"cell_type":"markdown","metadata":{"id":"LwfT57-uJIt9"},"source":["## Installation and imports"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"zDN04KYnHMmI","metadata":{}},"outputs":[],"source":["#@title Install required packages.\n","try:\n","    from google.colab import files  # checks if you are on google colab\n","    !rm -rf CogModelingRNNsTutorial\n","    !git clone https://github.com/whyhardt/CogModelingRNN.git\n","    %pip install -e CogModelingRNN/CogModelingRNNsTutorial\n","    !cp CogModelingRNN/CogModelingRNNsTutorial/*py CogModelingRNN\n","    %pip install pysindy\n","    _ON_COLAB = True\n","except:\n","    print('Not on Google Colab. Assuming you already installed the required packages.')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"lVdDzYwVHbdb","metadata":{}},"outputs":[],"source":["#@title Import libraries\n","import sys\n","import os\n","import warnings\n","from typing import Callable, Tuple, Iterable, Union\n","\n","import matplotlib.pyplot as plt\n","from sympy.parsing.sympy_parser import parse_expr\n","import numpy as np\n","import pandas as pd\n","import scipy.stats as st\n","import pickle\n","import torch\n","\n","import pysindy as ps\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","# RL libraries\n","sys.path.append('resources')  # add source directoy to path\n","from resources import bandits, rnn, rnn_utils, sindy_utils\n","\n","dict_agents = {\n","    'basic': lambda alpha, beta, n_actions, forgetting_rate, perseveration_bias: bandits.AgentQ(alpha, beta, n_actions, forgetting_rate, perseveration_bias),\n","    'quad_q': lambda alpha, beta, n_actions, forgetting_rate, perseveration_bias: bandits.AgentQuadQ(alpha, beta, n_actions, forgetting_rate, perseveration_bias)\n","}"]},{"cell_type":"markdown","metadata":{"id":"rCHCHSQbcJjU"},"source":["# RNN Reinforcement Learning"]},{"cell_type":"markdown","metadata":{"id":"Ca6uMC-Pglux"},"source":["## Set up agent and generate training data"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"hgqxccJZhT6d","metadata":{}},"outputs":[],"source":["#@title Select dataset type.\n","#@markdown ## Select dataset:\n","\n","dataset_type = 'synt'  #@param ['synt', 'real']\n","\n","#@markdown Set up parameters for synthetic data generation:\n","if dataset_type == 'synt':\n","    # agent parameters\n","    agent_kw = 'basic'  #@param ['basic', 'quad_q'] \n","    gen_alpha = .25 #@param\n","    gen_beta = 3 #@param\n","    forgetting_rate = 0.1 #@param\n","    perseveration_bias = 0.  #@param\n","    # environment parameters\n","    non_binary_reward = False #@param\n","    n_actions = 2 #@param\n","    sigma = .1  #@param\n","    \n","    # experiement parameters\n","    n_trials_per_session = 200  #@param\n","    n_sessions = 220  #@param\n","    \n","    # setup\n","    environment = bandits.EnvironmentBanditsDrift(sigma=sigma, n_actions=n_actions, non_binary_rewards=non_binary_reward)\n","    agent = dict_agents[agent_kw](gen_alpha, gen_beta, n_actions, forgetting_rate, perseveration_bias)  \n","  \n","    dataset_train, experiment_list_train = bandits.create_dataset(\n","        agent=agent,\n","        environment=environment,\n","        n_trials_per_session=n_trials_per_session,\n","        n_sessions=n_sessions)\n","\n","    dataset_test, experiment_list_test = bandits.create_dataset(\n","        agent=agent,\n","        environment=environment,\n","        n_trials_per_session=n_trials_per_session,\n","        n_sessions=n_sessions)\n","\n","#@markdown Set up parameters for loading rat data from Miller et al 2019.\n","elif dataset_type == 'real':\n","    # TODO: ys are not the rewards but the following choices!!!!\n","    raise NotImplementedError('This is not implemented yet.')\n","\n","    path = 'data/bahrami_100.csv'\n","    data = pd.read_csv(path)\n","    xs = data['action'].values\n","    ys = data['reward'].values\n","    episodes = np.unique(data['participant_id'].values)\n","    # reshape xs and ys to be (n_trials_per_episode, n_episodes, 1). Take the variable episodes as the index for the dim 'n_episodes'\n","    train_test_ratio = 0.8\n","    n_episodes_train = int(len(episodes)*train_test_ratio)\n","    n_episodes_test = len(episodes) - n_episodes_train\n","\n","    xs = xs.reshape(-1, len(episodes), 1)\n","    ys = ys.reshape(-1, len(episodes), 1)\n","    \n","    # one-hot encode xs\n","    xs = jax.nn.one_hot(xs[:, :, 0], num_classes=int(np.max(np.unique(xs[:, 0, 0])+1)))\n","    # delay xs by one time step to have previous choices\n","    xs = np.concatenate((np.zeros((1, *xs.shape[1:])), xs[:-1, :, :]), axis=0)\n","    # add one-time-step delayed reward as feature to xs\n","    reward_delayed = np.concatenate((np.zeros((1, *ys.shape[1:])), ys[:-1, :, :]), axis=0)\n","    xs = np.concatenate((xs, reward_delayed), axis=-1)\n","    \n","    xs_train = xs[:, :n_episodes_train]\n","    ys_train = ys[:, :n_episodes_train]\n","    xs_test = xs[:, n_episodes_train:]\n","    ys_test = ys[:, n_episodes_train:]\n","    \n","    n_actions = xs.shape[-1]# - 1  # -1 because of the delayed reward \n","    n_trials_per_session = xs.shape[0] \n","    n_sessions = xs_train.shape[1]\n","    \n","    dataset_train = rnn_utils.DatasetRNN(xs_train, ys_train)\n","    dataset_test = rnn_utils.DatasetRNN(xs_test, ys_test)\n","    \n","    experiment_list_train = None\n","    experiment_list_test = None\n","\n","else:\n","  raise NotImplementedError(\n","      (f'dataset_type {dataset_type} not implemented. '\n","       'Please select from drop-down list.'))"]},{"cell_type":"markdown","metadata":{"id":"t-wfvf86jgoU"},"source":["## Train SINDy on actual data and replace agent's update rule with SINDy update rule\n","\n","The target equation for SINDy with forgetting is:\n","\n","$$Q_\\text{k+1}=(1-f)Q_\\text{k} + f Q_0 - \\alpha (1-f) c Q_\\text{k} - \\alpha f Q_0 c + \\alpha c r$$\n","\n","For the values $f=0.1$, $\\alpha=0.25$, $Q_0=0.5$ this gives the constants\n","$$Q_\\text{k+1}=0.9 Q_\\text{k} + 0.05 - 0.225 c Q_\\text{k} -  0.0125 c + 0.25 c r$$"]},{"cell_type":"code","execution_count":null,"metadata":{"metadata":{}},"outputs":[],"source":["get_choices = True\n","poly_order = 3\n","threshold = 0.01\n","dt = 1\n","\n","custom_lib_functions = [\n","    # sub-library which is always included    \n","    lambda q,c,r: q,\n","    lambda q,c,r: r,\n","    lambda q,c,r: np.power(q, 2),\n","    lambda q,c,r: q*r,\n","    lambda q,c,r: np.power(r, 2),\n","    # sub-library if the possible action was chosen\n","    lambda q,c,r: c,\n","    lambda q,c,r: c*q,\n","    lambda q,c,r: c*r,\n","    lambda q,c,r: c*np.power(q, 2),\n","    lambda q,c,r: c*q*r,\n","    lambda q,c,r: c*np.power(r, 2),\n","]\n","\n","custom_lib_names = [\n","    # part library which is always included\n","    lambda q,c,r: f'{q}',\n","    lambda q,c,r: f'{r}',\n","    lambda q,c,r: f'{q}^2',\n","    lambda q,c,r: f'{q}*{r}',\n","    lambda q,c,r: f'{r}^2',\n","    # part library if the possible action was chosen\n","    lambda q,c,r: f'{c}',\n","    lambda q,c,r: f'{c}*{q}',\n","    lambda q,c,r: f'{c}*{r}',\n","    lambda q,c,r: f'{c}*{q}^2',\n","    lambda q,c,r: f'{c}*{q}*{r}',\n","    lambda q,c,r: f'{c}*{r}^2',\n","]\n","\n","# solution library for f=0.5, alpha=0.25, Q_init=0.5\n","# solution_lib = ps.CustomLibrary(\n","#     library_functions=[lambda q,c,r: 0.5*q + 0.25 - 0.125*c*q - 0.0675*c + 0.25*c*r],\n","#     function_names=[lambda q,c,r: f'0.5*q + 0.25 - 0.125*c*q - 0.0675*c + 0.25*c*r'],\n","#     include_bias=False,\n","#     library_ensemble=False,\n","# )"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40135,"status":"ok","timestamp":1703171428064,"user":{"displayName":"Daniel W","userId":"06430346412716090550"},"user_tz":-60},"id":"WgenO280YtSk","metadata":{},"outputId":"46beeeb1-ed36-493b-fd59-659c2bb98cee"},"outputs":[],"source":["#@title Fit SINDy to actual dataset\n","# library = custom_lib  # custom_lib, poly_lib, solution_lib\n","ensemble = False\n","library_ensemble = False\n","\n","# library_datasindy = ps.CustomLibrary(\n","#     library_functions=custom_lib_functions,\n","#     function_names=custom_lib_names,\n","#     include_bias=True,\n","# )\n","\n","library_datasindy = ps.PolynomialLibrary(poly_order)\n","\n","experiment_list_datasindy = None\n","\n","if dataset_type == 'synt':\n","    x_train, control, feature_names = sindy_utils.make_sindy_data(experiment_list_train, agent, get_choices=get_choices)\n","\n","    datasindy = ps.SINDy(\n","        optimizer=ps.STLSQ(threshold=threshold, verbose=True, alpha=0.1),\n","        feature_library=library_datasindy,\n","        discrete_time=True,\n","        feature_names=feature_names,\n","    )\n","    datasindy.fit(x_train, t=dt, u=control, ensemble=ensemble, library_ensemble=library_ensemble, multiple_trajectories=True)\n","    datasindy.print()\n","\n","    # set new sindy update rule and synthesize new dataset\n","    if not get_choices:\n","        update_rule_datasindy = lambda q, choice, reward: datasindy.simulate(q[choice], t=2, u=np.array(reward).reshape(1, 1))[-1]\n","    else:\n","        update_rule_datasindy = lambda q, choice, reward: datasindy.simulate(q, t=2, u=np.array([choice, reward]).reshape(1, 2))[-1]\n","    \n","    datasindyagent = bandits.AgentSindy(alpha=0, beta=gen_beta, n_actions=n_actions)\n","    datasindyagent.set_update_rule(update_rule_datasindy)\n","\n","    # _, experiment_list_datasindy = bandits.create_dataset(datasindyagent, environment, n_trials_per_session, n_sessions)"]},{"cell_type":"markdown","metadata":{},"source":["For the values $f=0.5$, $\\alpha=0.25$ and $Q_0=0.5$ the discovered model should be equal to\n","$$Q_\\text{k+1}=0.9 Q_\\text{k} + 0.05 - 0.225 c Q_\\text{k} -  0.0125 c + 0.25 c r$$"]},{"cell_type":"markdown","metadata":{"id":"5aYKermb0BJe"},"source":["## Fit a hybrid RNN and train SINDy on RNN dynamics"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"lkBYYdpXcO59","metadata":{}},"outputs":[],"source":["#@title Set up Hybrid RNN.\n","\n","#@markdown Is the model recurrent (ie can it see the hidden state from the previous step)\n","use_hidden_state = False  #@param ['True', 'False']\n","\n","#@markdown Is the model recurrent (ie can it see the hidden state from the previous step)\n","use_previous_values = False  #@param ['True', 'False']\n","\n","#@markdown If True, learn a value for the forgetting term\n","fit_forget = False  #@param ['True', 'False']\n","\n","#@markdown Learn a reward-independent term that depends on past choices.\n","habit_weight = \"0\"  #@param [0, 1]\n","habit_weight = float(habit_weight)\n","\n","value_weight = 1.  # This is needed for it to be doing RL\n","\n","rnn_rl_params = {\n","    's': use_hidden_state,\n","    'o': use_previous_values,\n","    'fit_forget': fit_forget,\n","    'forget': 0.,\n","    'w_h': habit_weight,\n","    'w_v': value_weight}\n","network_params = {'n_actions': n_actions, 'hidden_size': 16}\n","\n","model = rnn.RNN(\n","    n_actions=n_actions, \n","    hidden_size=16, \n","    init_value=0.5,\n","    use_habit=habit_weight==1,\n","    last_output=use_previous_values,\n","    last_state=use_hidden_state,\n","    )\n","\n","optimizer_rnn = torch.optim.Adam(model.parameters(), lr=1e-3)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":95493,"status":"ok","timestamp":1703173698779,"user":{"displayName":"Daniel W","userId":"06430346412716090550"},"user_tz":-60},"id":"catb-Attg4XL","metadata":{},"outputId":"67fc93c8-c51c-41bb-87ba-963fee64c698"},"outputs":[],"source":["train = True\n","load = False  # only relevant if train is True --> Determines whether to load trained parameters and continue training or start new training\n","\n","# params_path = 'params/params_rnn_forget_f01.pkl'\n","params_path = 'params/params_rnn_forget_f01_b3.pkl'\n","\n","if train:\n","  if load:\n","    # load trained parameters\n","    state_dict = torch.load(params_path)\n","    model.load_state_dict(state_dict['model'])\n","    optimizer_rnn.load_state_dict(state_dict['optimizer'])\n","    print('Loaded parameters.')\n","\n","  #@title Fit the hybrid RNN\n","  print('Training the hybrid RNN...')\n","  model, optimizer_rnn, _ = rnn_utils.fit_model(\n","      model=model,\n","      dataset=dataset_train,\n","      optimizer=optimizer_rnn,\n","      convergence_threshold=1e-5,\n","      n_steps_max=1000,\n","      batch_size=dataset_train._batch_size,\n","  )\n","\n","  # save trained parame\n","  # ters\n","  state_dict = {\n","    'model': model.state_dict(),\n","    'optimizer': optimizer_rnn.state_dict(),\n","  }\n","\n","else:\n","  # load trained parameters\n","  state_dict = torch.load(params_path)\n","  model.load_state_dict(state_dict['model'])\n","  print('Loaded parameters.')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"3e5500Whxah7","metadata":{}},"outputs":[],"source":["#@title Synthesize a dataset using the fitted network\n","hybrnn_agent = bandits.AgentNetwork_VisibleState(model, habit=habit_weight==1, n_actions=n_actions)\n","dataset_hybrnn, experiment_list_hybrnn = bandits.create_dataset(hybrnn_agent, environment, n_trials_per_session, int(n_sessions*1e-1))"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":63609,"status":"ok","timestamp":1703173770410,"user":{"displayName":"Daniel W","userId":"06430346412716090550"},"user_tz":-60},"id":"19B6ACNt1MhT","metadata":{},"outputId":"98b0ba77-336f-490c-da6f-5c1555a3da77"},"outputs":[],"source":["#@title Fit SINDy to RNN data and synthesize new dataset\n","\n","threshold = 0.015\n","\n","x_train, control, feature_names = sindy_utils.make_sindy_data(experiment_list_hybrnn, hybrnn_agent, get_choices=True)\n","# x_train, control, feature_names = sindy_utils.make_sindy_data(experiment_list_train, agent, get_choices=get_choices)\n","# scale q-values between 0 and 1 for more realistic dynamics\n","x_max = np.max(np.stack(x_train, axis=0))\n","x_min = np.min(np.stack(x_train, axis=0))\n","print(f'Dataset characteristics: max={x_max}, min={x_min}')\n","x_train = [(x - x_min) / (x_max - x_min) for x in x_train]\n","\n","# library_rnnsindy = ps.CustomLibrary(\n","#     library_functions=custom_lib_functions,\n","#     function_names=custom_lib_names,\n","#     include_bias=True,\n","# )\n","\n","library_rnnsindy = ps.PolynomialLibrary(poly_order)\n","\n","rnnsindy = ps.SINDy(\n","    optimizer=ps.STLSQ(threshold=threshold, verbose=False, alpha=0.1),\n","    feature_library=library_rnnsindy,\n","    discrete_time=True,\n","    feature_names=feature_names,\n",")\n","\n","rnnsindy.fit(x_train, t=dt, u=control, ensemble=True, library_ensemble=False, multiple_trajectories=True)\n","rnnsindy.print()\n","sparsity_index = np.sum(rnnsindy.coefficients() < threshold) / rnnsindy.coefficients().size\n","print(f'Sparsity index: {sparsity_index}')\n","\n","if not get_choices:\n","    update_rule_rnnsindy = lambda q, choice, reward: rnnsindy.simulate(q[choice], t=2, u=np.array(reward).reshape(1, 1))[-1]\n","else:\n","    update_rule_rnnsindy = lambda q, choice, reward: rnnsindy.simulate(q, t=2, u=np.array([choice, reward]).reshape(1, 2))[-1]\n","\n","rnnsindyagent = AgentSindy(alpha=0, beta=1, n_actions=n_actions)\n","rnnsindyagent.set_update_rule(update_rule_rnnsindy)"]},{"cell_type":"code","execution_count":null,"metadata":{"metadata":{}},"outputs":[],"source":["# groundtruth coefficients for model w/ and w/o forgetting; for polynomial order 3 library\n","groundtruth_coeffs = [forgetting_rate * 0.5, 1-forgetting_rate, -0.5*gen_alpha*forgetting_rate, 0, 0, -(1-forgetting_rate)*gen_alpha, 0, 0, gen_alpha, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","sindy_coeffs = rnnsindy.coefficients().reshape(-1).copy()\n","# post-processing of sindy coefficients\n","# sum up all coefficients that encode the same term if their values are equal\n","equal_terms = {'c': ['c', 'c^2', 'c^3'], 'r': ['r', 'r^2', 'r^3'], 'c r': ['c r', 'c^2 r', 'c r^2'], 'q c': ['q c', 'q c^2'], 'q r': ['q r', 'q r^2']}\n","sindy_terms = rnnsindy.get_feature_names()\n","if not non_binary_reward:\n","    for term in equal_terms.keys():\n","        for equal_term in equal_terms[term]:\n","            if equal_term in sindy_terms:\n","                if equal_term != term:\n","                    sindy_coeffs[sindy_terms.index(term)] += sindy_coeffs[sindy_terms.index(equal_term)]\n","                    sindy_coeffs[sindy_terms.index(equal_term)] = 0\n","\n","# filter all remaining coeffs which are lower than threshold\n","sindy_coeffs[np.abs(sindy_coeffs) < threshold] = 0\n","\n","list_coeffs = [[sindy_terms[i], groundtruth_coeffs[i], np.round(sindy_coeffs[i], 2)] for i in range(len(sindy_terms))]\n","list_features = ['term', 'groundtruth', 'sindy']\n","    \n","import pandas as pd\n","\n","pd.DataFrame(list_coeffs, columns=list_features).to_csv('params/params_recovery/recovered_coeffs_beta2_seqTraining.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# POST-PROCESSING\n","\n","# fit beta parameter of softmax by fitting on choice probability of the RNN by simple grid search\n","\n","# number of observed points\n","n_points = 100\n","\n","# epochs\n","epoch = 0\n","epochs_max = 100\n","session_id = 0\n","\n","# get choice probabilities of the RNN\n","qs, choice_probs_rnn = sindy_utils.get_q(experiment_list_hybrnn[session_id], hybrnn_agent)\n","\n","# set prior for beta parameter; x_max seems to be a good starting point\n","# beta_range = np.linspace(x_max-1, x_max+1, n_points)\n","beta_range = np.linspace(1, 10, n_points)\n","\n","# get choice probabilities of the SINDy agent for each beta in beta_range\n","choice_probs_sindy = np.zeros((len(beta_range), len(choice_probs_rnn), n_actions))\n","for i, beta in enumerate(beta_range):\n","    sindy_agent = AgentSindy(alpha=0, beta=beta, n_actions=n_actions)\n","    sindy_agent.set_update_rule(update_rule_rnnsindy)\n","    _, choice_probs_sindy_beta = sindy_utils.get_q(experiment_list_hybrnn[session_id], sindy_agent)\n","    \n","    # add choice probabilities to choice_probs_sindy\n","    choice_probs_sindy[i, :, :] = choice_probs_sindy_beta\n","    \n","# get best beta value by minimizing the error between choice probabilities of the RNN and the SINDy agent\n","errors = np.zeros(len(beta_range))\n","for i in range(len(beta_range)):\n","    errors[i] = np.sum(np.abs(choice_probs_rnn - choice_probs_sindy[i]))\n","\n","# get right beta value\n","beta = beta_range[np.argmin(errors)]\n","\n","# plot error plot with best beta value in title\n","plt.plot(beta_range, errors)\n","plt.title(f'Error plot with best beta={beta}')\n","plt.xlabel('Beta')\n","plt.ylabel('MAE')\n","plt.show()\n","\n","print(f'Setting up SINDy agent with beta={beta}...')\n","\n","rnnsindyagent = AgentSindy(alpha=0, beta=beta, n_actions=n_actions)\n","rnnsindyagent.set_update_rule(update_rule_rnnsindy)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# perform experiments with the SINDy agent\n","_, experiment_list_rnnsindy = bandits.create_dataset(rnnsindyagent, environment, n_trials_per_session, 1)#n_sessions)"]},{"cell_type":"markdown","metadata":{"id":"Onnp7JiYjuh4"},"source":["## Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["label_test, label_hybrnn, label_datasindy, label_rnnsindy = 'Test', 'Hybrid RNN', 'SINDy', 'RNN+SINDy'\n","\n","labels = [\n","    label_test, \n","    label_hybrnn, \n","    # label_datasindy, \n","    label_rnnsindy,\n","    ]\n","\n","save_fig = True\n","session_id = 0\n","binary = not non_binary_reward"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":2283,"status":"ok","timestamp":1703173772685,"user":{"displayName":"Daniel W","userId":"06430346412716090550"},"user_tz":-60},"id":"N03qCFykWOY1","outputId":"8e9ef522-efba-4009-dcae-6a695ac62f00"},"outputs":[],"source":["#@title Plot action similarities.\n","\n","# plot reward probabilities\n","choices = experiment_list_test[session_id].choices\n","rewards = experiment_list_test[session_id].rewards\n","\n","reward_probs = np.stack([experiment_list_test[session_id].timeseries[:, i] for i in range(n_actions)], axis=0)\n","bandits.plot_session(\n","    compare=True,\n","    choices=choices, \n","    rewards=rewards, \n","    timeseries=reward_probs,\n","    timeseries_name='', # 'Reward Probabilities'\n","    # labels=[f'Reward Prob {a}' for a in range(n_actions)],\n","    color=['tab:purple', 'tab:cyan'],\n","    binary=binary,\n","    )\n","plt.show() if not save_fig else plt.savefig('plots/reward_probs.png', dpi=1000)\n","\n","# plot evolution of Q-Values for same reward and choice trial data\n","\n","list_probs = []\n","list_qs = []\n","if label_test in labels:\n","    qs_test, probs_test = sindy_utils.get_q(experiment_list_test[session_id], agent)\n","    list_probs.append(np.expand_dims(probs_test, 0))\n","    list_qs.append(np.expand_dims(qs_test, 0))\n","if label_hybrnn in labels:\n","    qs_hybrnn, probs_hybrnn = sindy_utils.get_q(experiment_list_test[session_id], hybrnn_agent)\n","    list_probs.append(np.expand_dims(probs_hybrnn, 0))\n","    list_qs.append(np.expand_dims(qs_hybrnn, 0))\n","if label_datasindy in labels:\n","    qs_datasindy, probs_datasindy = sindy_utils.get_q(experiment_list_test[session_id], datasindyagent)\n","    list_probs.append(np.expand_dims(probs_datasindy, 0))\n","    list_qs.append(np.expand_dims(qs_datasindy, 0))\n","if label_rnnsindy in labels:\n","    qs_rnnsindy, probs_rnnsindy = sindy_utils.get_q(experiment_list_test[session_id], rnnsindyagent)\n","    list_probs.append(np.expand_dims(probs_rnnsindy, 0))\n","    list_qs.append(np.expand_dims(qs_rnnsindy, 0))\n","\n","# colors = ['cyan', 'magenta', 'yellow', 'grey']\n","colors = ['tab:blue', 'tab:orange', 'tab:pink', 'tab:gray']\n","\n","# concatenate all choice probs and q-values\n","probs = np.concatenate(list_probs, axis=0)\n","qs = np.concatenate(list_qs, axis=0)\n","\n","bandits.plot_session(\n","    compare=True,\n","    choices=choices,\n","    rewards=rewards,\n","    timeseries=probs[:, :, 0],\n","    timeseries_name='', # 'Choice Probabilities',\n","    # labels=labels,\n","    color=colors,\n","    binary=binary,\n","    )\n","plt.show() if not save_fig else plt.savefig('plots/choice_probs.png', dpi=1000)\n","\n","bandits.plot_session(\n","    compare=True,\n","    choices=choices,\n","    rewards=rewards,\n","    timeseries=qs[:, :, 0],\n","    timeseries_name='', # 'Q-Values',\n","    # labels=labels,\n","    color=colors,\n","    binary=binary,\n","    )\n","plt.show() if not save_fig else plt.savefig('plots/q_values.png', dpi=1000)\n","\n","def normalize(x, axis=1):\n","    x_min = np.min(x, keepdims=True, axis=axis)\n","    x_max = np.max(x, keepdims=True, axis=axis)\n","    return (x - x_min) / (x_max - x_min)\n","\n","qs_norm = normalize(qs)\n","\n","bandits.plot_session(\n","    compare=True,\n","    choices=choices,\n","    rewards=rewards,\n","    timeseries=qs_norm[:, :, 0],\n","    timeseries_name='', # 'norm. Q-Values',\n","    # labels=labels,\n","    color=colors,\n","    binary=binary,\n","    )\n","plt.show() if not save_fig else plt.savefig('plots/q_values_norm.png', dpi=1000)\n","\n","dqs_trials = np.diff(qs, axis=1)\n","# for i in range(1, len(qs)):\n","bandits.plot_session(\n","    compare=True,\n","    choices=choices,\n","    rewards=rewards,\n","    timeseries=dqs_trials[:, :, 0],\n","    timeseries_name='', # 'dQ/dt',\n","    # labels=labels,\n","    color=colors,\n","    binary=binary,\n",")\n","# plt.legend()\n","plt.show() if not save_fig else plt.savefig('plots/dq_dt.png', dpi=1000)\n","\n","norm_dqs_trials = normalize(dqs_trials)\n","bandits.plot_session(\n","    compare=True,\n","    choices=choices,\n","    rewards=rewards,\n","    timeseries=norm_dqs_trials[:, :, 0],\n","    timeseries_name='', # 'norm. dQ/dt',\n","    # labels=labels,\n","    color=colors,\n","    binary=binary,\n",")\n","# plt.legend()\n","plt.show() if not save_fig else plt.savefig('plots/dq_dt.png', dpi=1000)\n","\n","dqs_arms = np.diff(qs, axis=2)\n","norm_dqs_arms = normalize(dqs_arms)\n","# dqs_arms /= np.max(np.abs(dqs_arms), axis=(1, 2), keepdims=True)\n","bandits.plot_session(\n","    compare=True,\n","    choices=choices,\n","    rewards=rewards,\n","    timeseries=norm_dqs_arms,\n","    timeseries_name='', # 'dQ/dArm',\n","    # # labels=labels,\n","    color=colors,\n","    binary=binary,\n",")\n","plt.show() if not save_fig else plt.savefig('plots/dq_darm.png', dpi=1000)\n","\n","# Calculate reward rates\n","# Plot proportion Leftward Choices over difference in reward prob (left vs right)\n","\n","# experiment_list = []\n","# if label_test in labels:\n","#     print('Test dataset:')\n","#     bandits.show_total_reward_rate(experiment_list_test)\n","#     bandits.show_valuemetric(experiment_list_test, label=label_test)\n","#     experiment_list.append(experiment_list_test)\n","# if label_hybrnn in labels:\n","#     print('RNN dataset:')\n","#     bandits.show_total_reward_rate(experiment_list_hybrnn)\n","#     bandits.show_valuemetric(experiment_list_hybrnn, label=label_hybrnn)\n","#     experiment_list.append(experiment_list_hybrnn)\n","# if label_datasindy in labels:\n","#     print('Data SINDy dataset:')\n","#     bandits.show_total_reward_rate(experiment_list_datasindy)\n","#     bandits.show_valuemetric(experiment_list_datasindy, label=label_datasindy)\n","#     experiment_list.append(experiment_list_datasindy)\n","# if label_rnnsindy in labels:\n","#     print('RNN SINDy dataset:')\n","#     bandits.show_total_reward_rate(experiment_list_rnnsindy)\n","#     bandits.show_valuemetric(experiment_list_rnnsindy, label=label_rnnsindy)\n","#     experiment_list.append(experiment_list_rnnsindy)\n","# plt.legend()\n","# plt.show()\n","\n","# # plot choice similarity over history\n","# plt.figure()\n","# plot_action_similarity_to_history(experiment_list, n_steps_back=16, labels=labels, bbox_to_anchor=(1, 1))\n","# plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# check for correctly recovered parameters\n","# groundtruth equation: Q_k+1 = f Q_init + (1-f) Q_k - f * alpha * Q_init * c - (1-f) * alpha * c * Q_k + alpha * c * r\n","# equations = ['1','q','c','r','q^2','q c','q r','c^2','c r','r^2','q^3','q^2 c','q^2 r','q c^2','q c r','q r^2','c^3','c^2 r','c r^2','r^3']\n","# similar = [0, 1, 2, 3, 4, 5, 6, 2, 7, 3, 8, 9, 10, 5, ]\n","# groundtruth coefficients for model w/ and w/o forgetting; for polynomial order 3 library\n","groundtruth_coeffs = [forgetting_rate * 0.5, 1-forgetting_rate, -0.5*gen_alpha*forgetting_rate, 0, 0, -(1-forgetting_rate)*gen_alpha, 0, 0, gen_alpha, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","print('Groundtruth coefficients:')\n","print(groundtruth_coeffs)\n","sindy_coeffs = rnnsindy.coefficients().reshape(-1).copy()\n","print('raw SINDy coefficients:')\n","print(np.round(sindy_coeffs, 2))\n","# post-processing of sindy coefficients\n","# sum up all coefficients that encode the same term if their values are equal\n","equal_terms = {'c': ['c', 'c^2', 'c^3'], 'r': ['r', 'r^2', 'r^3'], 'c r': ['c r', 'c^2 r', 'c r^2'], 'q c': ['q c', 'q c^2'], 'q r': ['q r', 'q r^2']}\n","sindy_terms = rnnsindy.get_feature_names()\n","if not non_binary_reward:\n","    for term in equal_terms.keys():\n","        for equal_term in equal_terms[term]:\n","            if equal_term in sindy_terms:\n","                if equal_term != term:\n","                    sindy_coeffs[sindy_terms.index(term)] += sindy_coeffs[sindy_terms.index(equal_term)]\n","                    sindy_coeffs[sindy_terms.index(equal_term)] = 0\n","\n","print('post-processed SINDy coefficients:')\n","# filter all remaining coeffs which are lower than threshold\n","sindy_coeffs[np.abs(sindy_coeffs) < threshold] = 0\n","print(np.round(sindy_coeffs, 2))\n","\n","# get number of correctly recovered terms\n","correct_terms = 0\n","for i in range(len(sindy_terms)):\n","    if groundtruth_coeffs[i] != 0 and sindy_coeffs[i] != 0:\n","        correct_terms += 1\n","    elif groundtruth_coeffs[i] == 0 and sindy_coeffs[i] == 0:\n","        correct_terms += 1\n","\n","# substract the equal terms\n","substracted_terms = 0\n","if not non_binary_reward:\n","    for term in equal_terms.keys():\n","        if term in sindy_terms:\n","            substracted_terms += 1\n","print(f'Correctly recovered terms: {correct_terms-substracted_terms}/{len(sindy_terms)-substracted_terms}')\n","\n","# list_coeffs = [[sindy_terms[i], groundtruth_coeffs[i], np.round(sindy_coeffs[i], 2), np.round(rnnsindy.coefficients().reshape(-1)[i], 2)] for i in range(len(sindy_terms))]\n","# list_features = ['term', 'groundtruth', 'sindy', 'sindy_orig']\n","\n","list_coeffs = [[sindy_terms[i], groundtruth_coeffs[i], np.round(sindy_coeffs[i], 2)] for i in range(len(sindy_terms))]\n","list_features = ['term', 'groundtruth', 'sindy']\n","\n","print(list_features)\n","for i in range(len(list_coeffs)):\n","    print(list_coeffs[i])\n","    \n","import pandas as pd\n","\n","pd.DataFrame(list_coeffs, columns=list_features).to_csv('recovered_coeffs_beta'+str(gen_beta)+'.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def normalize(x, axis=1):\n","    x_min = np.min(x) #np.min(x, keepdims=True, axis=axis)\n","    x_max = np.max(x) #np.max(x, keepdims=True, axis=axis)\n","    return (x - x_min) / (x_max - x_min)\n","\n","# take qs_norm as q-values\n","experiment_dict = {\n","    label_test: experiment_list_test,\n","    label_hybrnn: experiment_list_hybrnn, \n","    # label_datasindy: experiment_list_datasindy, \n","    label_rnnsindy: experiment_list_rnnsindy,\n","    }\n","\n","# plot q-value update with old vs new q-values and reward as color\n","for l in experiment_dict.keys():\n","    qs = np.stack([experiment_dict[l][session].q for session in range(n_sessions)], axis=1)\n","    choices = np.stack([experiment_dict[l][session].choices for session in range(n_sessions)], axis=1)\n","    rewards = np.stack([experiment_dict[l][session].rewards for session in range(n_sessions)], axis=1)\n","    \n","    qs = normalize(qs, axis=0)\n","    fig = plt.figure()\n","    ax = fig.add_subplot(111)\n","    for session in range(n_sessions):\n","        ax.plot(np.linspace(-10, 10), np.linspace(-10, 10), 'grey', linewidth=0.5) \n","        for arm in range(n_actions):\n","            q_old = qs[:-1, session, arm]\n","            q_new = qs[1:, session, arm]\n","            ax.scatter(q_old, q_new, c=experiment_dict[l][session].rewards[:-1], alpha=.05, s=1)\n","    # set colorbar\n","    # cbar = plt.colorbar(ax.scatter([], [], c=[], alpha=1, s=1))\n","    # cbar.set_label('Reward')\n","    q_min = np.min(qs)\n","    q_max = np.max(qs)\n","    ax.set_ylim(q_min, q_max)\n","    ax.set_xlim(q_min, q_max)\n","    ax.set_xticks(np.linspace(q_min, q_max, 5))\n","    ax.set_yticks(np.linspace(q_min, q_max, 5))\n","    ax.set_xticklabels(['']*5)\n","    ax.set_yticklabels(['']*5)\n","    plt.rc('grid', color='grey')\n","    plt.grid()\n","    # plt.title(l)\n","    # plt.xlabel('Old Q-Values')\n","    # plt.ylabel('New Q-Values')\n","    plt.show() if not save_fig else plt.savefig(f'plots/q_value_update_{l}.png', dpi=1000)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNwCLLvTmMqZJNfKuTxnPyw","collapsed_sections":["LwfT57-uJIt9","Eq7jeg9mIx-f","Ca6uMC-Pglux","5aYKermb0BJe"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
