{"cells":[{"cell_type":"markdown","metadata":{"id":"LwfT57-uJIt9"},"source":["## Installation and imports"]},{"cell_type":"code","execution_count":74,"metadata":{"cellView":"form","id":"zDN04KYnHMmI"},"outputs":[{"name":"stdout","output_type":"stream","text":["Not on Google Colab. Assuming you already installed the required packages.\n"]}],"source":["#@title Install required packages.\n","try:\n","    from google.colab import files  # checks if you are on google colab\n","    !rm -rf CogModelingRNNsTutorial\n","    !git clone https://github.com/whyhardt/CogModelingRNN.git\n","    %pip install -e CogModelingRNN/CogModelingRNNsTutorial\n","    !cp CogModelingRNN/CogModelingRNNsTutorial/*py CogModelingRNN\n","    %pip install pysindy\n","    _ON_COLAB = True\n","except:\n","    print('Not on Google Colab. Assuming you already installed the required packages.')"]},{"cell_type":"code","execution_count":75,"metadata":{"cellView":"form","id":"lVdDzYwVHbdb"},"outputs":[],"source":["#@title Import libraries\n","import sys\n","import os\n","import warnings\n","from typing import Callable, Tuple, Iterable, Union\n","\n","import matplotlib.pyplot as plt\n","from sympy.parsing.sympy_parser import parse_expr\n","import numpy as np\n","import pandas as pd\n","import scipy.stats as st\n","import pickle\n","\n","# deepmind related libraries\n","import haiku as hk\n","import jax\n","import jax.numpy as jnp\n","import optax\n","\n","import pysindy as ps\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","# RL libraries\n","sys.path.append('resources')  # add source directoy to path\n","from resources import bandits, disrnn, hybrnn, hybrnn_forget, plotting, rat_data, rnn_utils"]},{"cell_type":"code","execution_count":77,"metadata":{"id":"0_eVhrNccDV7"},"outputs":[],"source":["#@title make update rule of Q-/SINDyNetwork-Agents adjustable and make values of RNN-Agent visible\n","\n","class AgentQuadQ(bandits.AgentQ):\n","  \n","  def __init__(\n","      self,\n","      alpha: float=0.2,\n","      beta: float=3.,\n","      n_actions: int=2,\n","      forgetting_rate: float=0.,\n","      perseveration_bias: float=0.,\n","      ):\n","    super().__init__(alpha, beta, n_actions, forgetting_rate, perseveration_bias)\n","  \n","  def update(self,\n","            choice: int,\n","            reward: float):\n","    \"\"\"Update the agent after one step of the task.\n","\n","    Args:\n","      choice: The choice made by the agent. 0 or 1\n","      reward: The reward received by the agent. 0 or 1\n","    \"\"\"\n","    \n","    # Decay q-values toward the initial value.\n","    self._q = (1-self._forgetting_rate) * self._q + self._forgetting_rate * self._q_init\n","\n","    # Update chosen q for chosen action with observed reward.\n","    self._q[choice] = self._q[choice] - self._alpha * self._q[choice]**2 + self._alpha * reward\n","\n","\n","class AgentSindy(bandits.AgentQ):\n","\n","  def __init__(\n","      self,\n","      alpha: float=0.2,\n","      beta: float=3.,\n","      n_actions: int=2,\n","      forgetting_rate: float=0.,\n","      perservation_bias: float=0.,):\n","    super().__init__(alpha, beta, n_actions, forgetting_rate, perservation_bias)\n","\n","    self._update_rule = lambda q, choice, reward: (1 - self._alpha) * q[choice] + self._alpha * reward\n","    self._update_rule_formula = None\n","\n","  def set_update_rule(self, update_rule: callable, update_rule_formula: str=None):\n","    self._update_rule=update_rule\n","    self._update_rule_formula=update_rule_formula\n","\n","  @property\n","  def update_rule(self):\n","    if self._update_rule_formula is not None:\n","      return self._update_rule_formula\n","    else:\n","      return f'{self._update_rule}'\n","\n","  def update(self, choice: int, reward: int):\n","\n","    for c in range(self._n_actions):\n","      self._q[c] = self._update_rule(self._q[c], int(c==choice), reward)\n","\n","\n","class AgentNetwork_VisibleState(bandits.AgentNetwork):\n","\n","  def __init__(self,\n","               make_network: Callable[[], hk.RNNCore],\n","               params: hk.Params,\n","               n_actions: int = 2,\n","               state_to_numpy: bool = False,\n","               habit=False):\n","    super().__init__(make_network=make_network, params=params, n_actions=n_actions, state_to_numpy=state_to_numpy)\n","    self.habit = habit\n","\n","  @property\n","  def q(self):\n","    if self.habit:\n","      return self._state[2], self._state[3]\n","    else:\n","      return self._state[3].reshape(-1)\n","\n","dict_agents = {\n","    'basic': lambda alpha, beta, n_actions, forgetting_rate, perseveration_bias: bandits.AgentQ(alpha, beta, n_actions, forgetting_rate, perseveration_bias),\n","    'quad_q': lambda alpha, beta, n_actions, forgetting_rate, perseveration_bias: AgentQuadQ(alpha, beta, n_actions, forgetting_rate, perseveration_bias)\n","}"]},{"cell_type":"code","execution_count":78,"metadata":{},"outputs":[],"source":["def get_q(experiment: bandits.BanditSession, agent: Union[bandits.AgentQ, bandits.AgentNetwork, AgentSindy]):\n","  \"\"\"Compute Q-Values of a specific agent for a specific experiment.\n","\n","  Args:\n","      experiment (bandits.BanditSession): _description_\n","      agent (_type_): _description_\n","\n","  Returns:\n","      _type_: _description_\n","  \"\"\"\n","  \n","  choices = np.expand_dims(experiment.choices, 1)\n","  rewards = np.expand_dims(experiment.rewards, 1)\n","  qs = np.zeros((experiment.choices.shape[0], agent._n_actions))\n","  choice_probs = np.zeros((experiment.choices.shape[0], agent._n_actions))\n","  \n","  agent.new_sess()\n","  \n","  for trial in range(experiment.choices.shape[0]):\n","    qs[trial] = agent.q\n","    choice_probs[trial] = agent.get_choice_probs()\n","    agent.update(int(choices[trial]), float(rewards[trial]))\n","    \n","  return qs, choice_probs\n","\n","\n","def parse_equation_for_sympy(eq):\n","    # replace all blank spaces with '*' where necessary\n","    # only between number and letter in exactly this order\n","    blanks = [i for i, ltr in enumerate(eq) if ltr == ' ']\n","    for blank in blanks:\n","        if (eq[blank+1].isalpha() or eq[blank-1].isdigit()) and (eq[blank+1].isalpha() or eq[blank+1].isdigit()):\n","            eq = eq[:blank] + '*' + eq[blank+1:]\n","    \n","    # replace all '^' with '**'\n","    eq = eq.replace('^', '**')\n","    \n","    # remove all [k]\n","    eq = eq.replace('[k]', '')\n","\n","    return eq\n","\n","def make_sindy_data(\n","    dataset,\n","    agent: bandits.AgentQ,\n","    sessions=-1,\n","    get_choices=True,\n","    # keep_sessions=False,\n","    ):\n","\n","  # Get training data for SINDy\n","  # put all relevant signals in x_train\n","\n","  if not isinstance(sessions, Iterable) and sessions == -1:\n","    # use all sessions\n","    sessions = np.arange(len(dataset))\n","  else:\n","    # use only the specified sessions\n","    sessions = np.array(sessions)\n","    \n","  if get_choices:\n","    n_control = 2\n","  else:\n","    n_control = 1\n","  \n","  # if keep_sessions:\n","  #   # concatenate all sessions along the trial dimensinon -> shape: (n_trials, n_sessions, n_features)\n","  #   choices = np.expand_dims(np.stack([dataset[i].choices for i in sessions], axis=1), -1)\n","  #   rewards = np.expand_dims(np.stack([dataset[i].rewards for i in sessions], axis=1), -1)\n","  #   qs = np.stack([dataset[i].q for i in sessions], axis=1)\n","  # else:\n","  # concatenate all sessions along the trial dimensinon -> shape: (n_trials*n_sessions, n_features)\n","  # choices = np.expand_dims(np.concatenate([dataset[i].choices for i in sessions], axis=0), -1)\n","  # rewards = np.expand_dims(np.concatenate([dataset[i].rewards for i in sessions], axis=0), -1)\n","  # qs = np.concatenate([dataset[i].q for i in sessions], axis=0)\n","  \n","  choices = np.stack([dataset[i].choices for i in sessions], axis=0)\n","  rewards = np.stack([dataset[i].rewards for i in sessions], axis=0)\n","  qs = np.stack([dataset[i].q for i in sessions], axis=0)\n","  \n","  if not get_choices:\n","    raise NotImplementedError('Only get_choices=True is implemented right now.')\n","    n_sessions = qs.shape[0]\n","    n_trials = qs.shape[1]*qs.shape[2]\n","    qs_all = np.zeros((n_sessions, n_trials))\n","    r_all = np.zeros((n_sessions, n_trials))\n","    c_all = None\n","    # concatenate the data of all arms into one array for more training data\n","    index_end_last_arm = 0\n","    for index_arm in range(agent._n_actions):\n","      index = np.where(choices==index_arm)[0]\n","      r_all[index_end_last_arm:index_end_last_arm+len(index)] = rewards[index]\n","      qs_all[index_end_last_arm:index_end_last_arm+len(index)] = qs[index, index_arm].reshape(-1, 1)\n","      index_end_last_arm += len(index)\n","  else:\n","    choices_oh = np.zeros((len(sessions), choices.shape[1], agent._n_actions))\n","    for sess in sessions:\n","      # one-hot encode choices\n","      choices_oh[sess] = np.eye(agent._n_actions)[choices[sess]]\n","      # add choices as control parameter; no sorting required then\n","      # qs_all = np.concatenate([qs[sess, :, i] for i in range(agent._n_actions)], axis=1)\n","      # c_all = np.concatenate([choices[:, sess, i] for i in range(agent._n_actions)], axis=1)\n","      # r_all = np.concatenate([rewards for _ in range(agent._n_actions)], axis=1)\n","      # concatenate all qs values of one sessions along the trial dimension\n","      qs_all = np.concatenate([np.stack([np.expand_dims(qs_sess[:, i], axis=-1) for i in range(agent._n_actions)], axis=0) for qs_sess in qs], axis=0)\n","      c_all = np.concatenate([np.stack([c_sess[:, i] for i in range(agent._n_actions)], axis=0) for c_sess in choices_oh], axis=0)\n","      r_all = np.concatenate([np.stack([r_sess for _ in range(agent._n_actions)], axis=0) for r_sess in rewards], axis=0)\n","  \n","  # get observed dynamics\n","  x_train = qs_all\n","  feature_names = ['q']\n","\n","  # get control\n","  control_names = []\n","  control = np.zeros((*x_train.shape[:-1], n_control))\n","  if get_choices:\n","    control[:, :, 0] = c_all\n","    control_names += ['c']\n","  control[:, :, n_control-1] = r_all\n","  control_names += ['r']\n","  \n","  feature_names += control_names\n","  \n","  print(f'Shape of Q-Values is: {x_train.shape}')\n","  print(f'Shape of control parameters is: {control.shape}')\n","  print(f'Feature names are: {feature_names}')\n","  \n","  # make x_train and control sequences instead of arrays\n","  x_train = [x_train_sess for x_train_sess in x_train]\n","  control = [control_sess for control_sess in control]\n"," \n","  return x_train, control, feature_names\n"]},{"cell_type":"markdown","metadata":{"id":"rCHCHSQbcJjU"},"source":["# RNN Reinforcement Learning"]},{"cell_type":"markdown","metadata":{"id":"Ca6uMC-Pglux"},"source":["## Set up agent and generate training data"]},{"cell_type":"code","execution_count":79,"metadata":{"cellView":"form","id":"hgqxccJZhT6d"},"outputs":[],"source":["#@title Select dataset type.\n","#@markdown ## Select dataset:\n","\n","dataset_type = 'synt'  #@param ['synt', 'real']\n","\n","#@markdown Set up parameters for synthetic data generation:\n","if dataset_type == 'synt':\n","    # agent parameters\n","    agent_kw = 'basic'  #@param ['basic', 'quad_q'] \n","    gen_alpha = .25 #@param\n","    gen_beta = 1 #@param\n","    forgetting_rate = 0.1 #@param\n","    perseveration_bias = 0.  #@param\n","    # environment parameters\n","    non_binary_reward = False #@param\n","    n_actions = 2 #@param\n","    sigma = .1  #@param\n","    \n","    # experiement parameters\n","    n_trials_per_session = 200  #@param\n","    n_sessions = 220  #@param\n","    \n","    # setup\n","    environment = bandits.EnvironmentBanditsDrift(sigma=sigma, n_actions=n_actions, non_binary_rewards=non_binary_reward)\n","    agent = dict_agents[agent_kw](gen_alpha, gen_beta, n_actions, forgetting_rate, perseveration_bias)  \n","\n","    dataset_test, experiment_list_test = bandits.create_dataset(\n","        agent=agent,\n","        environment=environment,\n","        n_trials_per_session=n_trials_per_session,\n","        n_sessions=n_sessions)\n","\n","#@markdown Set up parameters for loading rat data from Miller et al 2019.\n","elif dataset_type == 'real':\n","    # TODO: ys are not the rewards but the following choices!!!!\n","    raise NotImplementedError('This is not implemented yet.')\n","\n","    path = 'data/bahrami_100.csv'\n","    data = pd.read_csv(path)\n","    xs = data['action'].values\n","    ys = data['reward'].values\n","    episodes = np.unique(data['participant_id'].values)\n","    # reshape xs and ys to be (n_trials_per_episode, n_episodes, 1). Take the variable episodes as the index for the dim 'n_episodes'\n","    train_test_ratio = 0.8\n","    n_episodes_train = int(len(episodes)*train_test_ratio)\n","    n_episodes_test = len(episodes) - n_episodes_train\n","\n","    xs = xs.reshape(-1, len(episodes), 1)\n","    ys = ys.reshape(-1, len(episodes), 1)\n","    \n","    # one-hot encode xs\n","    xs = jax.nn.one_hot(xs[:, :, 0], num_classes=int(np.max(np.unique(xs[:, 0, 0])+1)))\n","    # delay xs by one time step to have previous choices\n","    xs = np.concatenate((np.zeros((1, *xs.shape[1:])), xs[:-1, :, :]), axis=0)\n","    # add one-time-step delayed reward as feature to xs\n","    reward_delayed = np.concatenate((np.zeros((1, *ys.shape[1:])), ys[:-1, :, :]), axis=0)\n","    xs = np.concatenate((xs, reward_delayed), axis=-1)\n","    \n","    xs_train = xs[:, :n_episodes_train]\n","    ys_train = ys[:, :n_episodes_train]\n","    xs_test = xs[:, n_episodes_train:]\n","    ys_test = ys[:, n_episodes_train:]\n","    \n","    n_actions = xs.shape[-1]# - 1  # -1 because of the delayed reward \n","    n_trials_per_session = xs.shape[0] \n","    n_sessions = xs_train.shape[1]\n","    \n","    dataset_train = rnn_utils.DatasetRNN(xs_train, ys_train)\n","    dataset_test = rnn_utils.DatasetRNN(xs_test, ys_test)\n","    \n","    experiment_list_train = None\n","    experiment_list_test = None\n","\n","else:\n","  raise NotImplementedError(\n","      (f'dataset_type {dataset_type} not implemented. '\n","       'Please select from drop-down list.'))"]},{"cell_type":"markdown","metadata":{},"source":["For the values $f=0.5$, $\\alpha=0.25$ and $Q_0=0.5$ the discovered model should be equal to\n","$$Q_\\text{k+1}=0.9 Q_\\text{k} + 0.05 - 0.225 c Q_\\text{k} -  0.0125 c + 0.25 c r$$"]},{"cell_type":"markdown","metadata":{"id":"5aYKermb0BJe"},"source":["## Fit a hybrid RNN and train SINDy on RNN dynamics"]},{"cell_type":"code","execution_count":82,"metadata":{"cellView":"form","id":"lkBYYdpXcO59"},"outputs":[],"source":["#@title Set up Hybrid RNN.\n","\n","#@markdown Is the model recurrent (ie can it see the hidden state from the previous step)\n","use_hidden_state = False  #@param ['True', 'False']\n","\n","#@markdown Is the model recurrent (ie can it see the hidden state from the previous step)\n","use_previous_values = False  #@param ['True', 'False']\n","\n","#@markdown If True, learn a value for the forgetting term\n","fit_forget = False  #@param ['True', 'False']\n","\n","#@markdown Learn a reward-independent term that depends on past choices.\n","habit_weight = \"0\"  #@param [0, 1]\n","habit_weight = float(habit_weight)\n","\n","value_weight = 1.  # This is needed for it to be doing RL\n","\n","rnn_rl_params = {\n","    's': use_hidden_state,\n","    'o': use_previous_values,\n","    'fit_forget': fit_forget,\n","    'forget': 0.,\n","    'w_h': habit_weight,\n","    'w_v': value_weight}\n","network_params = {'n_actions': n_actions, 'hidden_size': 16}\n","\n","def make_hybrnn():\n","  # model = hybrnn.BiRNN(rl_params=rnn_rl_params, network_params=network_params)\n","  model = hybrnn_forget.BiRNN(rl_params=rnn_rl_params, network_params=network_params)\n","  return model\n","\n","optimizer_rnn = optax.adam(learning_rate=1e-3)"]},{"cell_type":"code","execution_count":1,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":95493,"status":"ok","timestamp":1703173698779,"user":{"displayName":"Daniel W","userId":"06430346412716090550"},"user_tz":-60},"id":"catb-Attg4XL","outputId":"67fc93c8-c51c-41bb-87ba-963fee64c698"},"outputs":[{"ename":"NameError","evalue":"name 'bandits' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m params_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams/params_rnn_forget_f01_b\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(beta)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_model\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(i)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train:\n\u001b[1;32m     12\u001b[0m   \u001b[38;5;66;03m# set up env and agent\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m   environment \u001b[38;5;241m=\u001b[39m \u001b[43mbandits\u001b[49m\u001b[38;5;241m.\u001b[39mEnvironmentBanditsDrift(sigma\u001b[38;5;241m=\u001b[39msigma, n_actions\u001b[38;5;241m=\u001b[39mn_actions, non_binary_rewards\u001b[38;5;241m=\u001b[39mnon_binary_reward)\n\u001b[1;32m     14\u001b[0m   agent \u001b[38;5;241m=\u001b[39m dict_agents[agent_kw](gen_alpha, beta, n_actions, forgetting_rate, perseveration_bias)  \n\u001b[1;32m     15\u001b[0m   dataset_train, experiment_list_train \u001b[38;5;241m=\u001b[39m bandits\u001b[38;5;241m.\u001b[39mcreate_dataset(\n\u001b[1;32m     16\u001b[0m         agent\u001b[38;5;241m=\u001b[39magent,\n\u001b[1;32m     17\u001b[0m         environment\u001b[38;5;241m=\u001b[39menvironment,\n\u001b[1;32m     18\u001b[0m         n_trials_per_session\u001b[38;5;241m=\u001b[39mn_trials_per_session,\n\u001b[1;32m     19\u001b[0m         n_sessions\u001b[38;5;241m=\u001b[39mn_sessions)\n","\u001b[0;31mNameError\u001b[0m: name 'bandits' is not defined"]}],"source":["train = True\n","load = False  # only relevant if train is True --> Determines whether to load trained parameters and continue training or start new training\n","\n","\n","for beta in range(1, 6):\n","  for i in range(1,6):\n","    \n","    # params_path = 'params/params_rnn_forget_f01.pkl'\n","    params_path = 'params/params_rnn_forget_f01_b'+str(beta)+'_model'+str(i)+'.pkl'\n","\n","    if train:\n","      # set up env and agent\n","      environment = bandits.EnvironmentBanditsDrift(sigma=sigma, n_actions=n_actions, non_binary_rewards=non_binary_reward)\n","      agent = dict_agents[agent_kw](gen_alpha, beta, n_actions, forgetting_rate, perseveration_bias)  \n","      dataset_train, experiment_list_train = bandits.create_dataset(\n","            agent=agent,\n","            environment=environment,\n","            n_trials_per_session=n_trials_per_session,\n","            n_sessions=n_sessions)\n","      \n","      if load:\n","        with open(params_path, 'rb') as f:\n","          rnn_params = pickle.load(f)\n","        opt_state = rnn_params[1]\n","        rnn_params = rnn_params[0]\n","        print('Loaded parameters.')\n","      else:\n","        opt_state = None\n","        rnn_params = None\n","\n","      # with jax.disable_jit():\n","      #@title Fit the hybrid RNN\n","      print('Training the hybrid RNN...')\n","      rnn_params, opt_state, _ = rnn_utils.fit_model(\n","          model_fun=make_hybrnn,\n","          dataset=dataset_train,\n","          optimizer=optimizer_rnn,\n","          optimizer_state=opt_state,\n","          model_params=rnn_params,\n","          loss_fun='categorical',  # penalized_categorical, categorical\n","          convergence_thresh=1e-5,\n","          n_steps_max=1000,\n","      )\n","\n","      # save trained parameters\n","      params = (rnn_params, opt_state)\n","      with open(params_path, 'wb') as f:\n","        pickle.dump(params, f)\n","        \n","    else:\n","      # load trained parameters\n","      with open(params_path, 'rb') as f:\n","        rnn_params = pickle.load(f)[0]\n","      print('Loaded parameters.')\n","      \n","    #@title Synthesize a dataset using the fitted network\n","    hybrnn_agent = AgentNetwork_VisibleState(make_hybrnn, rnn_params, habit=habit_weight==1, n_actions=n_actions)\n","    dataset_hybrnn, experiment_list_hybrnn = bandits.create_dataset(hybrnn_agent, environment, n_trials_per_session, int(n_sessions*1e0))\n","\n","    #@title Fit SINDy to RNN data and synthesize new dataset\n","\n","    threshold = 0.015\n","    poly_order = 3\n","    dt = 1\n","\n","    x_train, control, feature_names = make_sindy_data(experiment_list_hybrnn, hybrnn_agent, get_choices=get_choices)\n","    # x_train, control, feature_names = make_sindy_data(experiment_list_train, agent, get_choices=get_choices)\n","    # scale q-values between 0 and 1 for more realistic dynamics\n","    x_max = np.max(np.stack(x_train, axis=0))\n","    x_min = np.min(np.stack(x_train, axis=0))\n","    print(f'Dataset characteristics: max={x_max}, min={x_min}')\n","    x_train = [(x - x_min) / (x_max - x_min) for x in x_train]\n","\n","    # library_rnnsindy = ps.CustomLibrary(\n","    #     library_functions=custom_lib_functions,\n","    #     function_names=custom_lib_names,\n","    #     include_bias=True,\n","    # )\n","\n","    library_rnnsindy = ps.PolynomialLibrary(poly_order)\n","\n","    rnnsindy = ps.SINDy(\n","        optimizer=ps.STLSQ(threshold=threshold, verbose=False, alpha=0.1),\n","        feature_library=library_rnnsindy,\n","        discrete_time=True,\n","        feature_names=feature_names,\n","    )\n","\n","    rnnsindy.fit(x_train, t=dt, u=control, ensemble=True, library_ensemble=False, multiple_trajectories=True)\n","\n","    # groundtruth coefficients for model w/ and w/o forgetting; for polynomial order 3 library\n","    groundtruth_coeffs = [forgetting_rate * 0.5, 1-forgetting_rate, -0.5*gen_alpha*forgetting_rate, 0, 0, -(1-forgetting_rate)*gen_alpha, 0, 0, gen_alpha, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","    sindy_coeffs = rnnsindy.coefficients().reshape(-1).copy()\n","    # post-processing of sindy coefficients\n","    # sum up all coefficients that encode the same term if their values are equal\n","    equal_terms = {'c': ['c', 'c^2', 'c^3'], 'r': ['r', 'r^2', 'r^3'], 'c r': ['c r', 'c^2 r', 'c r^2'], 'q c': ['q c', 'q c^2'], 'q r': ['q r', 'q r^2']}\n","    sindy_terms = rnnsindy.get_feature_names()\n","    if not non_binary_reward:\n","        for term in equal_terms.keys():\n","            for equal_term in equal_terms[term]:\n","                if equal_term in sindy_terms:\n","                    if equal_term != term:\n","                        sindy_coeffs[sindy_terms.index(term)] += sindy_coeffs[sindy_terms.index(equal_term)]\n","                        sindy_coeffs[sindy_terms.index(equal_term)] = 0\n","\n","    # filter all remaining coeffs which are lower than threshold\n","    sindy_coeffs[np.abs(sindy_coeffs) < threshold] = 0\n","\n","    list_coeffs = [[sindy_terms[i], groundtruth_coeffs[i], np.round(sindy_coeffs[i], 2)] for i in range(len(sindy_terms))]\n","    list_features = ['term', 'groundtruth', 'sindy']\n","        \n","    import pandas as pd\n","\n","    pd.DataFrame(list_coeffs, columns=list_features).to_csv('recovered_coeffs_beta'+str(gen_beta)+'.csv', index=False)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNwCLLvTmMqZJNfKuTxnPyw","collapsed_sections":["LwfT57-uJIt9","Eq7jeg9mIx-f","Ca6uMC-Pglux","5aYKermb0BJe"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
