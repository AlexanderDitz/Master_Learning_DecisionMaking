{"cells":[{"cell_type":"markdown","metadata":{"id":"LwfT57-uJIt9"},"source":["## Installation and imports"]},{"cell_type":"code","execution_count":1,"metadata":{"cellView":"form","id":"zDN04KYnHMmI","metadata":{}},"outputs":[{"name":"stdout","output_type":"stream","text":["Not on Google Colab. Assuming you already installed the required packages.\n"]}],"source":["#@title Install required packages.\n","try:\n","    from google.colab import files  # checks if you are on google colab\n","    !rm -rf CogModelingRNNsTutorial\n","    !git clone https://github.com/whyhardt/CogModelingRNN.git\n","    %pip install -e CogModelingRNN/CogModelingRNNsTutorial\n","    !cp CogModelingRNN/CogModelingRNNsTutorial/*py CogModelingRNN\n","    %pip install pysindy\n","    _ON_COLAB = True\n","except:\n","    print('Not on Google Colab. Assuming you already installed the required packages.')"]},{"cell_type":"code","execution_count":2,"metadata":{"cellView":"form","id":"lVdDzYwVHbdb","metadata":{}},"outputs":[],"source":["#@title Import libraries\n","import sys\n","import os\n","import warnings\n","from typing import Callable, Tuple, Iterable, Union\n","\n","import matplotlib.pyplot as plt\n","from sympy.parsing.sympy_parser import parse_expr\n","import numpy as np\n","import pandas as pd\n","import scipy.stats as st\n","import pickle\n","\n","# deepmind related libraries\n","import haiku as hk\n","import jax\n","import jax.numpy as jnp\n","import optax\n","\n","import pysindy as ps\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","# RL libraries\n","sys.path.append('resources')  # add source directoy to path\n","from resources import bandits_haiku, hybrnn_forget, rnn_utils_haiku"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"0_eVhrNccDV7","metadata":{}},"outputs":[],"source":["#@title make update rule of Q-/SINDyNetwork-Agents adjustable and make values of RNN-Agent visible\n","\n","class AgentQuadQ(bandits_haiku.AgentQ):\n","  \n","  def __init__(\n","      self,\n","      alpha: float=0.2,\n","      beta: float=3.,\n","      n_actions: int=2,\n","      forgetting_rate: float=0.,\n","      perseveration_bias: float=0.,\n","      ):\n","    super().__init__(alpha, beta, n_actions, forgetting_rate, perseveration_bias)\n","  \n","  def update(self,\n","            choice: int,\n","            reward: float):\n","    \"\"\"Update the agent after one step of the task.\n","\n","    Args:\n","      choice: The choice made by the agent. 0 or 1\n","      reward: The reward received by the agent. 0 or 1\n","    \"\"\"\n","    \n","    # Decay q-values toward the initial value.\n","    self._q = (1-self._forgetting_rate) * self._q + self._forgetting_rate * self._q_init\n","\n","    # Update chosen q for chosen action with observed reward.\n","    self._q[choice] = self._q[choice] - self._alpha * self._q[choice]**2 + self._alpha * reward\n","\n","\n","class AgentSindy(bandits_haiku.AgentQ):\n","\n","  def __init__(\n","      self,\n","      alpha: float=0.2,\n","      beta: float=3.,\n","      n_actions: int=2,\n","      forgetting_rate: float=0.,\n","      perservation_bias: float=0.,):\n","    super().__init__(alpha, beta, n_actions, forgetting_rate, perservation_bias)\n","\n","    self._update_rule = lambda q, choice, reward: (1 - self._alpha) * q[choice] + self._alpha * reward\n","    self._update_rule_formula = None\n","\n","  def set_update_rule(self, update_rule: callable, update_rule_formula: str=None):\n","    self._update_rule=update_rule\n","    self._update_rule_formula=update_rule_formula\n","\n","  @property\n","  def update_rule(self):\n","    if self._update_rule_formula is not None:\n","      return self._update_rule_formula\n","    else:\n","      return f'{self._update_rule}'\n","\n","  def update(self, choice: int, reward: int):\n","\n","    for c in range(self._n_actions):\n","      self._q[c] = self._update_rule(self._q[c], int(c==choice), reward)\n","\n","\n","class AgentNetwork_VisibleState(bandits_haiku.AgentNetwork):\n","\n","  def __init__(self,\n","               make_network: Callable[[], hk.RNNCore],\n","               params: hk.Params,\n","               n_actions: int = 2,\n","               state_to_numpy: bool = False,\n","               habit=False):\n","    super().__init__(make_network=make_network, params=params, n_actions=n_actions, state_to_numpy=state_to_numpy)\n","    self.habit = habit\n","\n","  @property\n","  def q(self):\n","    if self.habit:\n","      return self._state[2], self._state[3]\n","    else:\n","      return self._state[3].reshape(-1)\n","\n","dict_agents = {\n","    'basic': lambda alpha, beta, n_actions, forgetting_rate, perseveration_bias: bandits_haiku.AgentQ(alpha, beta, n_actions, forgetting_rate, perseveration_bias),\n","    'quad_q': lambda alpha, beta, n_actions, forgetting_rate, perseveration_bias: AgentQuadQ(alpha, beta, n_actions, forgetting_rate, perseveration_bias)\n","}"]},{"cell_type":"code","execution_count":4,"metadata":{"metadata":{}},"outputs":[],"source":["def get_q(experiment: bandits_haiku.BanditSession, agent: Union[bandits_haiku.AgentQ, bandits_haiku.AgentNetwork, AgentSindy]):\n","  \"\"\"Compute Q-Values of a specific agent for a specific experiment.\n","\n","  Args:\n","      experiment (bandits.BanditSession): _description_\n","      agent (_type_): _description_\n","\n","  Returns:\n","      _type_: _description_\n","  \"\"\"\n","  \n","  choices = np.expand_dims(experiment.choices, 1)\n","  rewards = np.expand_dims(experiment.rewards, 1)\n","  qs = np.zeros((experiment.choices.shape[0], agent._n_actions))\n","  choice_probs = np.zeros((experiment.choices.shape[0], agent._n_actions))\n","  \n","  agent.new_sess()\n","  \n","  for trial in range(experiment.choices.shape[0]):\n","    qs[trial] = agent.q\n","    choice_probs[trial] = agent.get_choice_probs()\n","    agent.update(int(choices[trial]), float(rewards[trial]))\n","    \n","  return qs, choice_probs\n","\n","\n","def parse_equation_for_sympy(eq):\n","    # replace all blank spaces with '*' where necessary\n","    # only between number and letter in exactly this order\n","    blanks = [i for i, ltr in enumerate(eq) if ltr == ' ']\n","    for blank in blanks:\n","        if (eq[blank+1].isalpha() or eq[blank-1].isdigit()) and (eq[blank+1].isalpha() or eq[blank+1].isdigit()):\n","            eq = eq[:blank] + '*' + eq[blank+1:]\n","    \n","    # replace all '^' with '**'\n","    eq = eq.replace('^', '**')\n","    \n","    # remove all [k]\n","    eq = eq.replace('[k]', '')\n","\n","    return eq\n","\n","def make_sindy_data(\n","    dataset,\n","    agent: bandits_haiku.AgentQ,\n","    sessions=-1,\n","    get_choices=True,\n","    # keep_sessions=False,\n","    ):\n","\n","  # Get training data for SINDy\n","  # put all relevant signals in x_train\n","\n","  if not isinstance(sessions, Iterable) and sessions == -1:\n","    # use all sessions\n","    sessions = np.arange(len(dataset))\n","  else:\n","    # use only the specified sessions\n","    sessions = np.array(sessions)\n","    \n","  if get_choices:\n","    n_control = 2\n","  else:\n","    n_control = 1\n","  \n","  # if keep_sessions:\n","  #   # concatenate all sessions along the trial dimensinon -> shape: (n_trials, n_sessions, n_features)\n","  #   choices = np.expand_dims(np.stack([dataset[i].choices for i in sessions], axis=1), -1)\n","  #   rewards = np.expand_dims(np.stack([dataset[i].rewards for i in sessions], axis=1), -1)\n","  #   qs = np.stack([dataset[i].q for i in sessions], axis=1)\n","  # else:\n","  # concatenate all sessions along the trial dimensinon -> shape: (n_trials*n_sessions, n_features)\n","  # choices = np.expand_dims(np.concatenate([dataset[i].choices for i in sessions], axis=0), -1)\n","  # rewards = np.expand_dims(np.concatenate([dataset[i].rewards for i in sessions], axis=0), -1)\n","  # qs = np.concatenate([dataset[i].q for i in sessions], axis=0)\n","  \n","  choices = np.stack([dataset[i].choices for i in sessions], axis=0)\n","  rewards = np.stack([dataset[i].rewards for i in sessions], axis=0)\n","  qs = np.stack([dataset[i].q for i in sessions], axis=0)\n","  \n","  if not get_choices:\n","    raise NotImplementedError('Only get_choices=True is implemented right now.')\n","    n_sessions = qs.shape[0]\n","    n_trials = qs.shape[1]*qs.shape[2]\n","    qs_all = np.zeros((n_sessions, n_trials))\n","    r_all = np.zeros((n_sessions, n_trials))\n","    c_all = None\n","    # concatenate the data of all arms into one array for more training data\n","    index_end_last_arm = 0\n","    for index_arm in range(agent._n_actions):\n","      index = np.where(choices==index_arm)[0]\n","      r_all[index_end_last_arm:index_end_last_arm+len(index)] = rewards[index]\n","      qs_all[index_end_last_arm:index_end_last_arm+len(index)] = qs[index, index_arm].reshape(-1, 1)\n","      index_end_last_arm += len(index)\n","  else:\n","    choices_oh = np.zeros((len(sessions), choices.shape[1], agent._n_actions))\n","    for sess in sessions:\n","      # one-hot encode choices\n","      choices_oh[sess] = np.eye(agent._n_actions)[choices[sess]]\n","      # add choices as control parameter; no sorting required then\n","      # qs_all = np.concatenate([qs[sess, :, i] for i in range(agent._n_actions)], axis=1)\n","      # c_all = np.concatenate([choices[:, sess, i] for i in range(agent._n_actions)], axis=1)\n","      # r_all = np.concatenate([rewards for _ in range(agent._n_actions)], axis=1)\n","      # concatenate all qs values of one sessions along the trial dimension\n","      qs_all = np.concatenate([np.stack([np.expand_dims(qs_sess[:, i], axis=-1) for i in range(agent._n_actions)], axis=0) for qs_sess in qs], axis=0)\n","      c_all = np.concatenate([np.stack([c_sess[:, i] for i in range(agent._n_actions)], axis=0) for c_sess in choices_oh], axis=0)\n","      r_all = np.concatenate([np.stack([r_sess for _ in range(agent._n_actions)], axis=0) for r_sess in rewards], axis=0)\n","  \n","  # get observed dynamics\n","  x_train = qs_all\n","  feature_names = ['q']\n","\n","  # get control\n","  control_names = []\n","  control = np.zeros((*x_train.shape[:-1], n_control))\n","  if get_choices:\n","    control[:, :, 0] = c_all\n","    control_names += ['c']\n","  control[:, :, n_control-1] = r_all\n","  control_names += ['r']\n","  \n","  feature_names += control_names\n","  \n","  print(f'Shape of Q-Values is: {x_train.shape}')\n","  print(f'Shape of control parameters is: {control.shape}')\n","  print(f'Feature names are: {feature_names}')\n","  \n","  # make x_train and control sequences instead of arrays\n","  x_train = [x_train_sess for x_train_sess in x_train]\n","  control = [control_sess for control_sess in control]\n"," \n","  return x_train, control, feature_names\n"]},{"cell_type":"markdown","metadata":{"id":"rCHCHSQbcJjU"},"source":["# RNN Reinforcement Learning"]},{"cell_type":"markdown","metadata":{"id":"Ca6uMC-Pglux"},"source":["## Set up agent and generate training data"]},{"cell_type":"code","execution_count":5,"metadata":{"cellView":"form","id":"hgqxccJZhT6d","metadata":{}},"outputs":[],"source":["#@title Select dataset type.\n","#@markdown ## Select dataset:\n","\n","dataset_type = 'synt'  #@param ['synt', 'real']\n","\n","#@markdown Set up parameters for synthetic data generation:\n","if dataset_type == 'synt':\n","    # agent parameters\n","    agent_kw = 'basic'  #@param ['basic', 'quad_q'] \n","    gen_alpha = .25 #@param\n","    gen_beta = 5 #@param\n","    forgetting_rate = 0 #@param\n","    perseveration_bias = 0.  #@param\n","    # environment parameters\n","    non_binary_reward = False #@param\n","    n_actions = 2 #@param\n","    sigma = .1  #@param\n","    \n","    # experiement parameters\n","    n_trials_per_session = 200  #@param\n","    n_sessions = 220  #@param\n","    \n","    # setup\n","    environment = bandits_haiku.EnvironmentBanditsDrift(sigma=sigma, n_actions=n_actions, non_binary_rewards=non_binary_reward)\n","    agent = dict_agents[agent_kw](gen_alpha, gen_beta, n_actions, forgetting_rate, perseveration_bias)  \n","  \n","    dataset_train, experiment_list_train = bandits_haiku.create_dataset(\n","        agent=agent,\n","        environment=environment,\n","        n_trials_per_session=n_trials_per_session,\n","        n_sessions=n_sessions)\n","\n","    dataset_test, experiment_list_test = bandits_haiku.create_dataset(\n","        agent=agent,\n","        environment=environment,\n","        n_trials_per_session=n_trials_per_session,\n","        n_sessions=n_sessions)\n","\n","#@markdown Set up parameters for loading rat data from Miller et al 2019.\n","elif dataset_type == 'real':\n","    # TODO: ys are not the rewards but the following choices!!!!\n","    raise NotImplementedError('This is not implemented yet.')\n","\n","    path = 'data/bahrami_100.csv'\n","    data = pd.read_csv(path)\n","    xs = data['action'].values\n","    ys = data['reward'].values\n","    episodes = np.unique(data['participant_id'].values)\n","    # reshape xs and ys to be (n_trials_per_episode, n_episodes, 1). Take the variable episodes as the index for the dim 'n_episodes'\n","    train_test_ratio = 0.8\n","    n_episodes_train = int(len(episodes)*train_test_ratio)\n","    n_episodes_test = len(episodes) - n_episodes_train\n","\n","    xs = xs.reshape(-1, len(episodes), 1)\n","    ys = ys.reshape(-1, len(episodes), 1)\n","    \n","    # one-hot encode xs\n","    xs = jax.nn.one_hot(xs[:, :, 0], num_classes=int(np.max(np.unique(xs[:, 0, 0])+1)))\n","    # delay xs by one time step to have previous choices\n","    xs = np.concatenate((np.zeros((1, *xs.shape[1:])), xs[:-1, :, :]), axis=0)\n","    # add one-time-step delayed reward as feature to xs\n","    reward_delayed = np.concatenate((np.zeros((1, *ys.shape[1:])), ys[:-1, :, :]), axis=0)\n","    xs = np.concatenate((xs, reward_delayed), axis=-1)\n","    \n","    xs_train = xs[:, :n_episodes_train]\n","    ys_train = ys[:, :n_episodes_train]\n","    xs_test = xs[:, n_episodes_train:]\n","    ys_test = ys[:, n_episodes_train:]\n","    \n","    n_actions = xs.shape[-1]# - 1  # -1 because of the delayed reward \n","    n_trials_per_session = xs.shape[0] \n","    n_sessions = xs_train.shape[1]\n","    \n","    dataset_train = rnn_utils_haiku.DatasetRNN(xs_train, ys_train)\n","    dataset_test = rnn_utils_haiku.DatasetRNN(xs_test, ys_test)\n","    \n","    experiment_list_train = None\n","    experiment_list_test = None\n","\n","else:\n","  raise NotImplementedError(\n","      (f'dataset_type {dataset_type} not implemented. '\n","       'Please select from drop-down list.'))"]},{"cell_type":"markdown","metadata":{"id":"t-wfvf86jgoU"},"source":["## Train SINDy on actual data and replace agent's update rule with SINDy update rule\n","\n","The target equation for SINDy with forgetting is:\n","\n","$$Q_\\text{k+1}=(1-f)Q_\\text{k} + f Q_0 - \\alpha (1-f) c Q_\\text{k} - \\alpha f Q_0 c + \\alpha c r$$\n","\n","For the values $f=0.1$, $\\alpha=0.25$, $Q_0=0.5$ this gives the constants\n","$$Q_\\text{k+1}=0.9 Q_\\text{k} + 0.05 - 0.225 c Q_\\text{k} -  0.0125 c + 0.25 c r$$"]},{"cell_type":"code","execution_count":6,"metadata":{"metadata":{}},"outputs":[],"source":["get_choices = True\n","poly_order = 3\n","threshold = 0.01\n","dt = 1\n","\n","custom_lib_functions = [\n","    # sub-library which is always included    \n","    lambda q,c,r: q,\n","    lambda q,c,r: r,\n","    lambda q,c,r: np.power(q, 2),\n","    lambda q,c,r: q*r,\n","    lambda q,c,r: np.power(r, 2),\n","    # sub-library if the possible action was chosen\n","    lambda q,c,r: c,\n","    lambda q,c,r: c*q,\n","    lambda q,c,r: c*r,\n","    lambda q,c,r: c*np.power(q, 2),\n","    lambda q,c,r: c*q*r,\n","    lambda q,c,r: c*np.power(r, 2),\n","]\n","\n","custom_lib_names = [\n","    # part library which is always included\n","    lambda q,c,r: f'{q}',\n","    lambda q,c,r: f'{r}',\n","    lambda q,c,r: f'{q}^2',\n","    lambda q,c,r: f'{q}*{r}',\n","    lambda q,c,r: f'{r}^2',\n","    # part library if the possible action was chosen\n","    lambda q,c,r: f'{c}',\n","    lambda q,c,r: f'{c}*{q}',\n","    lambda q,c,r: f'{c}*{r}',\n","    lambda q,c,r: f'{c}*{q}^2',\n","    lambda q,c,r: f'{c}*{q}*{r}',\n","    lambda q,c,r: f'{c}*{r}^2',\n","]\n","\n","# solution library for f=0.5, alpha=0.25, Q_init=0.5\n","# solution_lib = ps.CustomLibrary(\n","#     library_functions=[lambda q,c,r: 0.5*q + 0.25 - 0.125*c*q - 0.0675*c + 0.25*c*r],\n","#     function_names=[lambda q,c,r: f'0.5*q + 0.25 - 0.125*c*q - 0.0675*c + 0.25*c*r'],\n","#     include_bias=False,\n","#     library_ensemble=False,\n","# )"]},{"cell_type":"code","execution_count":7,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40135,"status":"ok","timestamp":1703171428064,"user":{"displayName":"Daniel W","userId":"06430346412716090550"},"user_tz":-60},"id":"WgenO280YtSk","metadata":{},"outputId":"46beeeb1-ed36-493b-fd59-659c2bb98cee"},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of Q-Values is: (440, 200, 1)\n","Shape of control parameters is: (440, 200, 2)\n","Feature names are: ['q', 'c', 'r']\n"," Iteration ... |y - Xw|^2 ...  a * |w|_2 ...      |w|_0 ... Total error: |y - Xw|^2 + a * |w|_2\n","         0 ... 2.7838e-02 ... 1.0496e-01 ...          6 ... 1.3280e-01\n","         1 ... 2.1280e-06 ... 1.0520e-01 ...          6 ... 1.0521e-01\n","(q)[k+1] = 1.000 q[k] + -0.125 q[k] c[k] + 0.083 c[k] r[k] + -0.125 q[k] c[k]^2 + 0.083 c[k]^2 r[k] + 0.083 c[k] r[k]^2\n"]}],"source":["#@title Fit SINDy to actual dataset\n","# library = custom_lib  # custom_lib, poly_lib, solution_lib\n","ensemble = False\n","library_ensemble = False\n","\n","# library_datasindy = ps.CustomLibrary(\n","#     library_functions=custom_lib_functions,\n","#     function_names=custom_lib_names,\n","#     include_bias=True,\n","# )\n","\n","library_datasindy = ps.PolynomialLibrary(poly_order)\n","\n","experiment_list_datasindy = None\n","\n","if dataset_type == 'synt':\n","    x_train, control, feature_names = make_sindy_data(experiment_list_train, agent, get_choices=get_choices)\n","\n","    datasindy = ps.SINDy(\n","        optimizer=ps.STLSQ(threshold=threshold, verbose=True, alpha=0.1),\n","        feature_library=library_datasindy,\n","        discrete_time=True,\n","        feature_names=feature_names,\n","    )\n","    datasindy.fit(x_train, t=dt, u=control, ensemble=ensemble, library_ensemble=library_ensemble, multiple_trajectories=True)\n","    datasindy.print()\n","\n","    # set new sindy update rule and synthesize new dataset\n","    if not get_choices:\n","        update_rule_datasindy = lambda q, choice, reward: datasindy.simulate(q[choice], t=2, u=np.array(reward).reshape(1, 1))[-1]\n","    else:\n","        update_rule_datasindy = lambda q, choice, reward: datasindy.simulate(q, t=2, u=np.array([choice, reward]).reshape(1, 2))[-1]\n","    \n","    datasindyagent = AgentSindy(alpha=0, beta=gen_beta, n_actions=n_actions)\n","    datasindyagent.set_update_rule(update_rule_datasindy)\n","\n","    # _, experiment_list_datasindy = bandits.create_dataset(datasindyagent, environment, n_trials_per_session, n_sessions)"]},{"cell_type":"markdown","metadata":{},"source":["For the values $f=0.5$, $\\alpha=0.25$ and $Q_0=0.5$ the discovered model should be equal to\n","$$Q_\\text{k+1}=0.9 Q_\\text{k} + 0.05 - 0.225 c Q_\\text{k} -  0.0125 c + 0.25 c r$$"]},{"cell_type":"markdown","metadata":{"id":"5aYKermb0BJe"},"source":["## Fit a hybrid RNN and train SINDy on RNN dynamics"]},{"cell_type":"code","execution_count":8,"metadata":{"cellView":"form","id":"lkBYYdpXcO59","metadata":{}},"outputs":[],"source":["#@title Set up Hybrid RNN.\n","\n","#@markdown Is the model recurrent (ie can it see the hidden state from the previous step)\n","use_hidden_state = False  #@param ['True', 'False']\n","\n","#@markdown Is the model recurrent (ie can it see the hidden state from the previous step)\n","use_previous_values = False  #@param ['True', 'False']\n","\n","#@markdown If True, learn a value for the forgetting term\n","fit_forget = False  #@param ['True', 'False']\n","\n","#@markdown Learn a reward-independent term that depends on past choices.\n","habit_weight = \"0\"  #@param [0, 1]\n","habit_weight = float(habit_weight)\n","\n","value_weight = 1.  # This is needed for it to be doing RL\n","\n","rnn_rl_params = {\n","    's': use_hidden_state,\n","    'o': use_previous_values,\n","    'fit_forget': fit_forget,\n","    'forget': 0.,\n","    'w_h': habit_weight,\n","    'w_v': value_weight}\n","network_params = {'n_actions': n_actions, 'hidden_size': 16}\n","\n","def make_hybrnn():\n","  # model = hybrnn.BiRNN(rl_params=rnn_rl_params, network_params=network_params)\n","  model = hybrnn_forget.BiRNN(rl_params=rnn_rl_params, network_params=network_params)\n","  return model\n","\n","optimizer_rnn = optax.adam(learning_rate=1e-3)"]},{"cell_type":"code","execution_count":9,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":95493,"status":"ok","timestamp":1703173698779,"user":{"displayName":"Daniel W","userId":"06430346412716090550"},"user_tz":-60},"id":"catb-Attg4XL","metadata":{},"outputId":"67fc93c8-c51c-41bb-87ba-963fee64c698"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training the hybrid RNN...\n"]},{"name":"stderr","output_type":"stream","text":["2024-05-16 20:13:40.744898: W external/xla/xla/service/gpu/nvptx_compiler.cc:760] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.3.107). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"]},{"name":"stdout","output_type":"stream","text":["Step 500 of 500; Loss: 0.4279804; Time: 10.4s)\n","Model not yet converged - Running more steps of gradient descent. Time elapsed = 1e-05s.\n","Step 500 of 500; Loss: 0.4255578; Time: 9.6s)\n","Model not yet converged (convergence_value = 0.005660473) - Running more steps of gradient descent. Time elapsed = 2e-05s.\n","Step 500 of 500; Loss: 0.4255193; Time: 9.5s)\n","Model not yet converged (convergence_value = 9.048031e-05) - Running more steps of gradient descent. Time elapsed = 3e-05s.\n","Step 500 of 500; Loss: 0.4254587; Time: 9.6s)\n","Model not yet converged (convergence_value = 0.0001423863) - Running more steps of gradient descent. Time elapsed = 4e-05s.\n","Step 500 of 500; Loss: 0.4253564; Time: 9.5s)\n","Model not yet converged (convergence_value = 0.0002405431) - Running more steps of gradient descent. Time elapsed = 2e-05s.\n","Step 240 of 500; Loss: 0.4252789; Time: 5.0s)"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[9], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# with jax.disable_jit():\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#@title Fit the hybrid RNN\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining the hybrid RNN...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m rnn_params, opt_state, _ \u001b[38;5;241m=\u001b[39m \u001b[43mrnn_utils_haiku\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_fun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_hybrnn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_rnn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrnn_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcategorical\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# penalized_categorical, categorical\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvergence_thresh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_steps_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# save trained parameters\u001b[39;00m\n\u001b[1;32m     33\u001b[0m params \u001b[38;5;241m=\u001b[39m (rnn_params, opt_state)\n","File \u001b[0;32m~/repositories/closedloop_rl/resources/rnn_utils_haiku.py:325\u001b[0m, in \u001b[0;36mfit_model\u001b[0;34m(model_fun, dataset, optimizer, model_params, optimizer_state, loss_fun, convergence_thresh, random_key, n_steps_per_call, n_steps_max, batch_size)\u001b[0m\n\u001b[1;32m    323\u001b[0m n_calls_to_train_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m continue_training:\n\u001b[0;32m--> 325\u001b[0m   params, opt_state, losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel_fun\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m      \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m      \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m      \u001b[49m\u001b[43mloss_fun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fun\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdo_plot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m      \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_steps_per_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m   n_calls_to_train_model \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    336\u001b[0m   t_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n","File \u001b[0;32m~/repositories/closedloop_rl/resources/rnn_utils_haiku.py:246\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model_fun, dataset, optimizer, random_key, opt_state, params, n_steps, penalty_scale, loss_fun, do_plot, truncate_seq_length)\u001b[0m\n\u001b[1;32m    243\u001b[0m   xs \u001b[38;5;241m=\u001b[39m xs[:truncate_seq_length]\n\u001b[1;32m    244\u001b[0m   ys \u001b[38;5;241m=\u001b[39m ys[:truncate_seq_length]\n\u001b[0;32m--> 246\u001b[0m loss, params, opt_state \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_i\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# Log every 10th step\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m9\u001b[39m:\n","File \u001b[0;32m<string>:1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(_cls, count, mu, nu)\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["train = True\n","load = False  # only relevant if train is True --> Determines whether to load trained parameters and continue training or start new training\n","\n","# params_path = 'params/params_rnn_forget_f01.pkl'\n","params_path = 'params/params_rnn_forget_f0_b5.pkl'\n","\n","if train:\n","  if load:\n","    with open(params_path, 'rb') as f:\n","      rnn_params = pickle.load(f)\n","    opt_state = rnn_params[1]\n","    rnn_params = rnn_params[0]\n","    print('Loaded parameters.')\n","  else:\n","    opt_state = None\n","    rnn_params = None\n","\n","  # with jax.disable_jit():\n","  #@title Fit the hybrid RNN\n","  print('Training the hybrid RNN...')\n","  rnn_params, opt_state, _ = rnn_utils_haiku.fit_model(\n","      model_fun=make_hybrnn,\n","      dataset=dataset_train,\n","      optimizer=optimizer_rnn,\n","      optimizer_state=opt_state,\n","      model_params=rnn_params,\n","      loss_fun='categorical',  # penalized_categorical, categorical\n","      convergence_thresh=1e-5,\n","      n_steps_max=10000,\n","  )\n","\n","  # save trained parameters\n","  params = (rnn_params, opt_state)\n","  with open(params_path, 'wb') as f:\n","    pickle.dump(params, f)\n","    \n","else:\n","  # load trained parameters\n","  with open(params_path, 'rb') as f:\n","    rnn_params = pickle.load(f)[0]\n","  print('Loaded parameters.')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"3e5500Whxah7","metadata":{}},"outputs":[],"source":["#@title Synthesize a dataset using the fitted network\n","hybrnn_agent = AgentNetwork_VisibleState(make_hybrnn, rnn_params, habit=habit_weight==1, n_actions=n_actions)\n","dataset_hybrnn, experiment_list_hybrnn = bandits_haiku.create_dataset(hybrnn_agent, environment, n_trials_per_session, int(n_sessions*1e-1))"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":63609,"status":"ok","timestamp":1703173770410,"user":{"displayName":"Daniel W","userId":"06430346412716090550"},"user_tz":-60},"id":"19B6ACNt1MhT","metadata":{},"outputId":"98b0ba77-336f-490c-da6f-5c1555a3da77"},"outputs":[],"source":["#@title Fit SINDy to RNN data and synthesize new dataset\n","\n","threshold = 0.015\n","\n","x_train, control, feature_names = make_sindy_data(experiment_list_hybrnn, hybrnn_agent, get_choices=True)\n","# x_train, control, feature_names = make_sindy_data(experiment_list_train, agent, get_choices=get_choices)\n","# scale q-values between 0 and 1 for more realistic dynamics\n","x_max = np.max(np.stack(x_train, axis=0))\n","x_min = np.min(np.stack(x_train, axis=0))\n","print(f'Dataset characteristics: max={x_max}, min={x_min}')\n","x_train = [(x - x_min) / (x_max - x_min) for x in x_train]\n","\n","# library_rnnsindy = ps.CustomLibrary(\n","#     library_functions=custom_lib_functions,\n","#     function_names=custom_lib_names,\n","#     include_bias=True,\n","# )\n","\n","library_rnnsindy = ps.PolynomialLibrary(poly_order)\n","\n","rnnsindy = ps.SINDy(\n","    optimizer=ps.STLSQ(threshold=threshold, verbose=False, alpha=0.1),\n","    feature_library=library_rnnsindy,\n","    discrete_time=True,\n","    feature_names=feature_names,\n",")\n","\n","rnnsindy.fit(x_train, t=dt, u=control, ensemble=True, library_ensemble=False, multiple_trajectories=True)\n","rnnsindy.print()\n","sparsity_index = np.sum(rnnsindy.coefficients() < threshold) / rnnsindy.coefficients().size\n","print(f'Sparsity index: {sparsity_index}')\n","\n","if not get_choices:\n","    update_rule_rnnsindy = lambda q, choice, reward: rnnsindy.simulate(q[choice], t=2, u=np.array(reward).reshape(1, 1))[-1]\n","else:\n","    update_rule_rnnsindy = lambda q, choice, reward: rnnsindy.simulate(q, t=2, u=np.array([choice, reward]).reshape(1, 2))[-1]\n","\n","rnnsindyagent = AgentSindy(alpha=0, beta=1, n_actions=n_actions)\n","rnnsindyagent.set_update_rule(update_rule_rnnsindy)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# POST-PROCESSING\n","\n","# fit beta parameter of softmax by fitting on choice probability of the RNN by simple grid search\n","\n","# number of observed points\n","n_points = 100\n","\n","# epochs\n","epoch = 0\n","epochs_max = 100\n","session_id = 0\n","\n","# get choice probabilities of the RNN\n","qs, choice_probs_rnn = get_q(experiment_list_hybrnn[session_id], hybrnn_agent)\n","\n","# set prior for beta parameter; x_max seems to be a good starting point\n","# beta_range = np.linspace(x_max-1, x_max+1, n_points)\n","beta_range = np.linspace(1, 10, n_points)\n","\n","# get choice probabilities of the SINDy agent for each beta in beta_range\n","choice_probs_sindy = np.zeros((len(beta_range), len(choice_probs_rnn), n_actions))\n","for i, beta in enumerate(beta_range):\n","    sindy_agent = AgentSindy(alpha=0, beta=beta, n_actions=n_actions)\n","    sindy_agent.set_update_rule(update_rule_rnnsindy)\n","    _, choice_probs_sindy_beta = get_q(experiment_list_hybrnn[session_id], sindy_agent)\n","    \n","    # add choice probabilities to choice_probs_sindy\n","    choice_probs_sindy[i, :, :] = choice_probs_sindy_beta\n","    \n","# get best beta value by minimizing the error between choice probabilities of the RNN and the SINDy agent\n","errors = np.zeros(len(beta_range))\n","for i in range(len(beta_range)):\n","    errors[i] = np.sum(np.abs(choice_probs_rnn - choice_probs_sindy[i]))\n","\n","# get right beta value\n","beta = beta_range[np.argmin(errors)]\n","\n","# plot error plot with best beta value in title\n","plt.plot(beta_range, errors)\n","plt.title(f'Error plot with best beta={beta}')\n","plt.xlabel('Beta')\n","plt.ylabel('MAE')\n","plt.show()\n","\n","print(f'Setting up SINDy agent with beta={beta}...')\n","\n","rnnsindyagent = AgentSindy(alpha=0, beta=beta, n_actions=n_actions)\n","rnnsindyagent.set_update_rule(update_rule_rnnsindy)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# perform experiments with the SINDy agent\n","_, experiment_list_rnnsindy = bandits_haiku.create_dataset(rnnsindyagent, environment, n_trials_per_session, 1)#n_sessions)"]},{"cell_type":"markdown","metadata":{"id":"Onnp7JiYjuh4"},"source":["## Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["label_test, label_hybrnn, label_datasindy, label_rnnsindy = 'Test', 'Hybrid RNN', 'SINDy', 'RNN+SINDy'\n","\n","labels = [\n","    label_test, \n","    label_hybrnn, \n","    # label_datasindy, \n","    label_rnnsindy,\n","    ]\n","\n","save_fig = True\n","session_id = 0\n","binary = not non_binary_reward"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":2283,"status":"ok","timestamp":1703173772685,"user":{"displayName":"Daniel W","userId":"06430346412716090550"},"user_tz":-60},"id":"N03qCFykWOY1","outputId":"8e9ef522-efba-4009-dcae-6a695ac62f00"},"outputs":[],"source":["#@title Plot action similarities.\n","\n","# plot reward probabilities\n","choices = experiment_list_test[session_id].choices\n","rewards = experiment_list_test[session_id].rewards\n","\n","reward_probs = np.stack([experiment_list_test[session_id].timeseries[:, i] for i in range(n_actions)], axis=0)\n","bandits_haiku.plot_session(\n","    compare=True,\n","    choices=choices, \n","    rewards=rewards, \n","    timeseries=reward_probs,\n","    timeseries_name='', # 'Reward Probabilities'\n","    # labels=[f'Reward Prob {a}' for a in range(n_actions)],\n","    color=['tab:purple', 'tab:cyan'],\n","    binary=binary,\n","    )\n","plt.show() if not save_fig else plt.savefig('plots/reward_probs.png', dpi=1000)\n","\n","# plot evolution of Q-Values for same reward and choice trial data\n","\n","list_probs = []\n","list_qs = []\n","if label_test in labels:\n","    qs_test, probs_test = get_q(experiment_list_test[session_id], agent)\n","    list_probs.append(np.expand_dims(probs_test, 0))\n","    list_qs.append(np.expand_dims(qs_test, 0))\n","if label_hybrnn in labels:\n","    qs_hybrnn, probs_hybrnn = get_q(experiment_list_test[session_id], hybrnn_agent)\n","    list_probs.append(np.expand_dims(probs_hybrnn, 0))\n","    list_qs.append(np.expand_dims(qs_hybrnn, 0))\n","if label_datasindy in labels:\n","    qs_datasindy, probs_datasindy = get_q(experiment_list_test[session_id], datasindyagent)\n","    list_probs.append(np.expand_dims(probs_datasindy, 0))\n","    list_qs.append(np.expand_dims(qs_datasindy, 0))\n","if label_rnnsindy in labels:\n","    qs_rnnsindy, probs_rnnsindy = get_q(experiment_list_test[session_id], rnnsindyagent)\n","    list_probs.append(np.expand_dims(probs_rnnsindy, 0))\n","    list_qs.append(np.expand_dims(qs_rnnsindy, 0))\n","\n","# colors = ['cyan', 'magenta', 'yellow', 'grey']\n","colors = ['tab:blue', 'tab:orange', 'tab:pink', 'tab:gray']\n","\n","# concatenate all choice probs and q-values\n","probs = np.concatenate(list_probs, axis=0)\n","qs = np.concatenate(list_qs, axis=0)\n","\n","bandits_haiku.plot_session(\n","    compare=True,\n","    choices=choices,\n","    rewards=rewards,\n","    timeseries=probs[:, :, 0],\n","    timeseries_name='Probability',\n","    title='Given and estimated choice probabilities in a two-armed bandit task',\n","    labels=labels,\n","    color=colors,\n","    binary=binary,\n","    axis_info=True,\n","    )\n","plt.show() if not save_fig else plt.savefig('plots/choice_probs.png', dpi=1000)\n","\n","bandits_haiku.plot_session(\n","    compare=True,\n","    choices=choices,\n","    rewards=rewards,\n","    timeseries=qs[:, :, 0],\n","    timeseries_name='', # 'Q-Values',\n","    # labels=labels,\n","    color=colors,\n","    binary=binary,\n","    )\n","plt.show() if not save_fig else plt.savefig('plots/q_values.png', dpi=1000)\n","\n","def normalize(x, axis=1):\n","    x_min = np.min(x, keepdims=True, axis=axis)\n","    x_max = np.max(x, keepdims=True, axis=axis)\n","    return (x - x_min) / (x_max - x_min)\n","\n","qs_norm = normalize(qs)\n","\n","bandits_haiku.plot_session(\n","    compare=True,\n","    choices=choices,\n","    rewards=rewards,\n","    timeseries=qs_norm[:, :, 0],\n","    timeseries_name='', # 'norm. Q-Values',\n","    # labels=labels,\n","    color=colors,\n","    binary=binary,\n","    )\n","plt.show() if not save_fig else plt.savefig('plots/q_values_norm.png', dpi=1000)\n","\n","dqs_trials = np.diff(qs, axis=1)\n","# for i in range(1, len(qs)):\n","bandits_haiku.plot_session(\n","    compare=True,\n","    choices=choices,\n","    rewards=rewards,\n","    timeseries=dqs_trials[:, :, 0],\n","    timeseries_name='', # 'dQ/dt',\n","    # labels=labels,\n","    color=colors,\n","    binary=binary,\n",")\n","# plt.legend()\n","plt.show() if not save_fig else plt.savefig('plots/dq_dt.png', dpi=1000)\n","\n","norm_dqs_trials = normalize(dqs_trials)\n","bandits_haiku.plot_session(\n","    compare=True,\n","    choices=choices,\n","    rewards=rewards,\n","    timeseries=norm_dqs_trials[:, :, 0],\n","    timeseries_name='', # 'norm. dQ/dt',\n","    # labels=labels,\n","    color=colors,\n","    binary=binary,\n",")\n","# plt.legend()\n","plt.show() if not save_fig else plt.savefig('plots/dq_dt.png', dpi=1000)\n","\n","dqs_arms = np.diff(qs, axis=2)\n","norm_dqs_arms = normalize(dqs_arms)\n","# dqs_arms /= np.max(np.abs(dqs_arms), axis=(1, 2), keepdims=True)\n","bandits_haiku.plot_session(\n","    compare=True,\n","    choices=choices,\n","    rewards=rewards,\n","    timeseries=norm_dqs_arms,\n","    timeseries_name='', # 'dQ/dArm',\n","    # # labels=labels,\n","    color=colors,\n","    binary=binary,\n",")\n","plt.show() if not save_fig else plt.savefig('plots/dq_darm.png', dpi=1000)\n","\n","# Calculate reward rates\n","# Plot proportion Leftward Choices over difference in reward prob (left vs right)\n","\n","# experiment_list = []\n","# if label_test in labels:\n","#     print('Test dataset:')\n","#     bandits.show_total_reward_rate(experiment_list_test)\n","#     bandits.show_valuemetric(experiment_list_test, label=label_test)\n","#     experiment_list.append(experiment_list_test)\n","# if label_hybrnn in labels:\n","#     print('RNN dataset:')\n","#     bandits.show_total_reward_rate(experiment_list_hybrnn)\n","#     bandits.show_valuemetric(experiment_list_hybrnn, label=label_hybrnn)\n","#     experiment_list.append(experiment_list_hybrnn)\n","# if label_datasindy in labels:\n","#     print('Data SINDy dataset:')\n","#     bandits.show_total_reward_rate(experiment_list_datasindy)\n","#     bandits.show_valuemetric(experiment_list_datasindy, label=label_datasindy)\n","#     experiment_list.append(experiment_list_datasindy)\n","# if label_rnnsindy in labels:\n","#     print('RNN SINDy dataset:')\n","#     bandits.show_total_reward_rate(experiment_list_rnnsindy)\n","#     bandits.show_valuemetric(experiment_list_rnnsindy, label=label_rnnsindy)\n","#     experiment_list.append(experiment_list_rnnsindy)\n","# plt.legend()\n","# plt.show()\n","\n","# # plot choice similarity over history\n","# plt.figure()\n","# plot_action_similarity_to_history(experiment_list, n_steps_back=16, labels=labels, bbox_to_anchor=(1, 1))\n","# plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# check for correctly recovered parameters\n","# groundtruth equation: Q_k+1 = f Q_init + (1-f) Q_k - f * alpha * Q_init * c - (1-f) * alpha * c * Q_k + alpha * c * r\n","# equations = ['1','q','c','r','q^2','q c','q r','c^2','c r','r^2','q^3','q^2 c','q^2 r','q c^2','q c r','q r^2','c^3','c^2 r','c r^2','r^3']\n","# similar = [0, 1, 2, 3, 4, 5, 6, 2, 7, 3, 8, 9, 10, 5, ]\n","# groundtruth coefficients for model w/ and w/o forgetting; for polynomial order 3 library\n","groundtruth_coeffs = [forgetting_rate * 0.5, 1-forgetting_rate, -0.5*gen_alpha*forgetting_rate, 0, 0, -(1-forgetting_rate)*gen_alpha, 0, 0, gen_alpha, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","print('Groundtruth coefficients:')\n","print(groundtruth_coeffs)\n","sindy_coeffs = rnnsindy.coefficients().reshape(-1).copy()\n","print('raw SINDy coefficients:')\n","print(np.round(sindy_coeffs, 2))\n","# post-processing of sindy coefficients\n","# sum up all coefficients that encode the same term if their values are equal\n","equal_terms = {'c': ['c', 'c^2', 'c^3'], 'r': ['r', 'r^2', 'r^3'], 'c r': ['c r', 'c^2 r', 'c r^2'], 'q c': ['q c', 'q c^2'], 'q r': ['q r', 'q r^2']}\n","sindy_terms = rnnsindy.get_feature_names()\n","if not non_binary_reward:\n","    for term in equal_terms.keys():\n","        for equal_term in equal_terms[term]:\n","            if equal_term in sindy_terms:\n","                if equal_term != term:\n","                    sindy_coeffs[sindy_terms.index(term)] += sindy_coeffs[sindy_terms.index(equal_term)]\n","                    sindy_coeffs[sindy_terms.index(equal_term)] = 0\n","\n","print('post-processed SINDy coefficients:')\n","# filter all remaining coeffs which are lower than threshold\n","sindy_coeffs[np.abs(sindy_coeffs) < threshold] = 0\n","print(np.round(sindy_coeffs, 2))\n","\n","# get number of correctly recovered terms\n","correct_terms = 0\n","for i in range(len(sindy_terms)):\n","    if groundtruth_coeffs[i] != 0 and sindy_coeffs[i] != 0:\n","        correct_terms += 1\n","    elif groundtruth_coeffs[i] == 0 and sindy_coeffs[i] == 0:\n","        correct_terms += 1\n","\n","# substract the equal terms\n","substracted_terms = 0\n","if not non_binary_reward:\n","    for term in equal_terms.keys():\n","        if term in sindy_terms:\n","            substracted_terms += 1\n","print(f'Correctly recovered terms: {correct_terms-substracted_terms}/{len(sindy_terms)-substracted_terms}')\n","\n","# list_coeffs = [[sindy_terms[i], groundtruth_coeffs[i], np.round(sindy_coeffs[i], 2), np.round(rnnsindy.coefficients().reshape(-1)[i], 2)] for i in range(len(sindy_terms))]\n","# list_features = ['term', 'groundtruth', 'sindy', 'sindy_orig']\n","\n","list_coeffs = [[sindy_terms[i], groundtruth_coeffs[i], np.round(sindy_coeffs[i], 2)] for i in range(len(sindy_terms))]\n","list_features = ['term', 'groundtruth', 'sindy']\n","\n","print(list_features)\n","for i in range(len(list_coeffs)):\n","    print(list_coeffs[i])\n","    \n","import pandas as pd\n","\n","pd.DataFrame(list_coeffs, columns=list_features).to_csv('recovered_coeffs_beta'+str(gen_beta)+'.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def normalize(x, axis=1):\n","    x_min = np.min(x) #np.min(x, keepdims=True, axis=axis)\n","    x_max = np.max(x) #np.max(x, keepdims=True, axis=axis)\n","    return (x - x_min) / (x_max - x_min)\n","\n","# take qs_norm as q-values\n","experiment_dict = {\n","    label_test: experiment_list_test,\n","    label_hybrnn: experiment_list_hybrnn, \n","    # label_datasindy: experiment_list_datasindy, \n","    label_rnnsindy: experiment_list_rnnsindy,\n","    }\n","\n","# plot q-value update with old vs new q-values and reward as color\n","for l in experiment_dict.keys():\n","    qs = np.stack([experiment_dict[l][session].q for session in range(n_sessions)], axis=1)\n","    choices = np.stack([experiment_dict[l][session].choices for session in range(n_sessions)], axis=1)\n","    rewards = np.stack([experiment_dict[l][session].rewards for session in range(n_sessions)], axis=1)\n","    \n","    qs = normalize(qs, axis=0)\n","    fig = plt.figure()\n","    ax = fig.add_subplot(111)\n","    for session in range(n_sessions):\n","        ax.plot(np.linspace(-10, 10), np.linspace(-10, 10), 'grey', linewidth=0.5) \n","        for arm in range(n_actions):\n","            q_old = qs[:-1, session, arm]\n","            q_new = qs[1:, session, arm]\n","            ax.scatter(q_old, q_new, c=experiment_dict[l][session].rewards[:-1], alpha=.05, s=1)\n","    # set colorbar\n","    # cbar = plt.colorbar(ax.scatter([], [], c=[], alpha=1, s=1))\n","    # cbar.set_label('Reward')\n","    q_min = np.min(qs)\n","    q_max = np.max(qs)\n","    ax.set_ylim(q_min, q_max)\n","    ax.set_xlim(q_min, q_max)\n","    ax.set_xticks(np.linspace(q_min, q_max, 5))\n","    ax.set_yticks(np.linspace(q_min, q_max, 5))\n","    ax.set_xticklabels(['']*5)\n","    ax.set_yticklabels(['']*5)\n","    plt.rc('grid', color='grey')\n","    plt.grid()\n","    # plt.title(l)\n","    # plt.xlabel('Old Q-Values')\n","    # plt.ylabel('New Q-Values')\n","    plt.show() if not save_fig else plt.savefig(f'plots/q_value_update_{l}.png', dpi=1000)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNwCLLvTmMqZJNfKuTxnPyw","collapsed_sections":["LwfT57-uJIt9","Eq7jeg9mIx-f","Ca6uMC-Pglux","5aYKermb0BJe"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
